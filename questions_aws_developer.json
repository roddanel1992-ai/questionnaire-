[
  {
    "question": "A developer is incorporating AWS X-Ray into an application that handles personal identifiable information (PII). The application is hosted on Amazon EC2 instances. The application trace messages include encrypted PII and go to Amazon CloudWatch. The developer needs to ensure that no PII goes outside of the EC2 instances. Which solution will meet these requirements?",
    "options": [
      "Manually instrument the X-Ray SDK in the application code.",
      "Use the X-Ray auto-instrumentation agent.",
      "Use Amazon Macie to detect and hide PI",
      "Call the X-Ray API from AWS Lambda.",
      "Use AWS Distro for Open Telemetry."
    ],
    "answer": "Manually instrument the X-Ray SDK in the application code.",
    "explanation": "This solution will meet the requirements by allowing the developer to control what data is sent to X-Ray and CloudWatch from the application code. The developer can filter out any PII from the trace messages before sending them to X-Ray and CloudWatch, ensuring that no PII goes outside of the EC2 instances. Option B is not optimal because it will automatically instrument all incoming and outgoing requests from the application, which may include PII in the trace messages. Option C is not optimal because it will require additional services and costs to use Amazon Macie and AWS Lambda, which may not be able to detect and hide all PII from the trace messages. Option D is not optimal because it will use Open Telemetry instead of X-Ray, which may not be compatible with CloudWatch and other AWS services."
  },
  {
    "question": "A developer is creating a mobile app that calls a backend service by using an Amazon API Gateway REST API. For integration testing during the development phase, the developer wants to simulate different backend responses without invoking the backend service. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "Create an AWS Lambda functio",
      "Use API Gateway proxy integration to return constant HTTP responses.",
      "Create an Amazon EC2 instance that serves the backend REST API by using an AWS CloudFormation template.",
      "Customize the API Gateway stage to select a response type based on the request.",
      "Use a request mapping template to select the mock integration response."
    ],
    "answer": "Customize the API Gateway stage to select a response type based on the request.",
    "explanation": "Amazon API Gateway supports mock integration responses, which are predefined responses that can be returned without sending requests to a backend service. Mock integration responses can be used for testing or prototyping purposes, or for simulating different backend responses based on certain conditions. A request mapping template can be used to select a mock integration response based on an expression that evaluates some aspects of the request, such as headers, query strings, or body content. This solution does not require any additional resources or code changes and has the least operational overhead. Reference: Set up mock integrations for an API Gateway REST API https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-mock- integration.html"
  },
  {
    "question": "A developer is deploying a company's application to Amazon EC2 instances The application generates gigabytes of data files each day The files are rarely accessed but the files must be available to the application's users within minutes of a request during the first year of storage The company must retain the files for 7 years. How can the developer implement the application to meet these requirements MOST cost- effectively?",
    "options": [
      "Store the files in an Amazon S3 bucket Use the S3 Glacier Instant Retrieval storage class Create an S3 Lifecycle policy to transition the files to the S3 Glacier Deep Archive storage class after 1 year",
      "Store the files in an Amazon S3 bucke",
      "Use the S3 Standard storage clas",
      "Create an S3 Lifecycle policy to transition the files to the S3 Glacier Flexible Retrieval storage class after 1 year.",
      "Store the files on an Amazon Elastic Block Store (Amazon EBS) volume Use Amazon Data Lifecycle Manager (Amazon DLM) to create snapshots of the EBS volumes and to store those snapshots in Amazon S3. Store the files on an Amazon Elastic File System (Amazon EFS) moun. Configure EFS lifecycle management to transition the files to the EFS Standard-Infrequent Access (Standard-IA) storage class after 1 year."
    ],
    "answer": "Store the files in an Amazon S3 bucket Use the S3 Glacier Instant Retrieval storage class Create an S3 Lifecycle policy to transition the files to the S3 Glacier Deep Archive storage class after 1 year",
    "explanation": "Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds. With S3 Glacier Instant Retrieval, you can save up to 68% on storage costs compared to using the S3 Standard- Infrequent Access (S3 Standard-IA) storage class, when your data is accessed once per quarter. https://aws.amazon.com/s3/storage- classes/glacier/instant- retrieval/"
  },
  {
    "question": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments. How should the developer retrieve the variables with the FEWEST application changes?",
    "options": [
      "Update the application to retrieve the variables from AWS Systems Manager Parameter Stor",
      "Use unique paths in Parameter Store for each variable in each environmen",
      "Store the credentials in AWS Secrets Manager in each environment.",
      "Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment.",
      "Update the application to retrieve the variables from an encrypted file that is stored with the applicatio. Store the API URL and credentials in unique files for each environment.. Update the application to retrieve the variables from each of the deployed environment. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process."
    ],
    "answer": "Update the application to retrieve the variables from AWS Systems Manager Parameter Stor",
    "explanation": "AWS Systems Manager Parameter Store is a service that provides secure, hierarchical storage for configuration data management and secrets management. The developer can update the application to retrieve the variables from Parameter Store by using the AWS SDK or the AWS CLI. The developer can use unique paths in Parameter Store for each variable in each environment, such as /dev/api-url, /test/api-url, and /prod/api-url. The developer can also store the credentials in AWS Secrets Manager, which is integrated with Parameter Store and provides additional features such as automatic rotation and encryption."
  },
  {
    "question": "A developer is testing a RESTful application that is deployed by using Amazon API Gateway and AWS Lambda When the developer tests the user login by using credentials that are not valid, the developer receives an HTTP 405 METHOD_NOT_ALLOWED error The developer has verified that the test is sending the correct request for the resource Which HTTP error should the application return in response to the request?",
    "options": [
      "HTTP 401",
      "HTTP 404",
      "HTTP 503",
      "HTTP 505"
    ],
    "answer": "HTTP 401",
    "explanation": "The HTTP 401 error indicates that the request has not been applied because it lacks valid authentication credentials for the target resource. This is the appropriate error code to return when the user login fails due to invalid credentials. The HTTP 405 error means that the method specified in the request is not allowed for the resource identified by the request URI, which is not the case here. The other error codes are not relevant to the authentication failure scenario. References ? HTTP Status Codes ? AWS Lambda Function Errors in API Gateway"
  },
  {
    "question": "An online food company provides an Amazon API Gateway HTTP API 1o receive orders for partners. The API is integrated with an AWS Lambda function. The Lambda function stores the orders in an Amazon DynamoDB table. The company expects to onboard additional partners Some to me panthers require additional Lambda function to receive orders. The company has created an Amazon S3 bucket. The company needs 10 store all orders and updates m the S3 bucket for future analysis How can the developer ensure that an orders and updates are stored to Amazon S3 with the LEAST development effort?",
    "options": [
      "Create a new Lambda function and a new API Gateway API endpoin",
      "Configure the new Lambda function to write to the S3 bucke",
      "Modify the original Lambda function to post updates to the new API endpoint.",
      "Use Amazon Kinesis Data Streams to create a new data strea"
    ],
    "answer": "Modify the original Lambda function to post updates to the new API endpoint.",
    "explanation": "This solution will ensure that all orders and updates are stored to Amazon S3 with the least development effort because it uses DynamoDB Streams to capture changes in the DynamoDB table and trigger a Lambda function to write those changes to the S3 bucket. This way, the original Lambda function and API Gateway API endpoint do not need to be modified, and no additional services are required. Option A is not optimal because it will require more development effort to create a new Lambda function and a new API Gateway API endpoint, and to modify the original Lambda function to post updates to the new API endpoint. Option B is not optimal because it will introduce additional costs and complexity to use Amazon Kinesis Data Streams to create a new data stream, and to modify the Lambda function to publish orders to the data stream. Option D is not optimal because it will require more development effort to modify the Lambda function to publish to a new Amazon SNS topic, and to create and subscr"
  },
  {
    "question": "A company notices that credentials that the company uses to connect to an external software as a service (SaaS) vendor are stored in a configuration file as plaintext. The developer needs to secure the API credentials and enforce automatic credentials rotation on a quarterly basis. Which solution will meet these requirements MOST securely?",
    "options": [
      "Use AWS Key Management Service (AWS KMS) to encrypt the configuration fil",
      "Decrypt the configuration file when users make API calls to the SaaS vendo",
      "Enable rotation.",
      "Retrieve temporary credentials from AWS Security Token Service (AWS STS) every 15 minute"
    ],
    "answer": "Enable rotation.",
    "explanation": "Store the credentials in AWS Secrets Manager and enable rotation. Configure the API to have Secrets Manager access. This is correct. This solution will meet the requirements most securely, because it uses a service that is designed to store and manage secrets such as API credentials. AWS Secrets Manager helps you protect access to your applications, services, and IT resources by enabling you to rotate, manage, and retrieve secrets throughout their lifecycle1. You can store secrets such as passwords, database strings, API keys, and license codes as encrypted values2. You can also configure automatic rotation of your secrets on a schedule that you specify3. You can use the AWS SDK or CLI to retrieve secrets from Secrets Manager when you need them4. This way, you can avoid storing credentials in plaintext files or hardcoding them in your code."
  },
  {
    "question": "A company has a multi-node Windows legacy application that runs on premises. The application uses a network shared folder as a centralized configuration repository to store configuration files in .xml format. The company is migrating the application to Amazon EC2 instances. As part of the migration to AWS, a developer must identify a solution that provides high availability for the repository. Which solution will meet this requirement MOST cost-effectively?",
    "options": [
      "Mount an Amazon Elastic Block Store (Amazon EBS) volume onto one of the EC2 instance",
      "Deploy a file system on the EBS volum",
      "Use the host operating system to share a folde shared folder.",
      "Update the application code to read and write configuration files from the"
    ],
    "answer": "Use the host operating system to share a folde shared folder.",
    "explanation": "Amazon S3 is a service that provides highly scalable, durable, and secure object storage. The developer can create an S3 bucket to host the repository and migrate the existing .xml files to the S3 bucket. The developer can update the application code to use the AWS SDK to read and write configuration files from S3. This solution will meet the requirement of high availability for the repository in a cost-effective way."
  },
  {
    "question": "A developer is troubleshooting an Amazon API Gateway API Clients are receiving HTTP 400 response errors when the clients try to access an endpoint of the API. How can the developer determine the cause of these errors?",
    "options": [
      "Create an Amazon Kinesis Data Firehose delivery stream to receive API call logs from API Gatewa",
      "Configure Amazon CloudWatch Logs as the delivery stream's destination.",
      "Turn on AWS CloudTrail Insights and create a trail Specify the Amazon Resource Name (ARN) of the trail for the stage of the API. Turn on AWS X-Ray for the API stage Create an Amazon CtoudWalch Logs log group Specify the Amazon Resource Name (ARN)",
      "of the log group for the API stage.",
      "Turn on execution logging and access logging in Amazon CloudWatch Logs for the API stag. Create a CloudWatch Logs log grou. Specify the Amazon Resource Name (ARN) of the log group for the API stage."
    ],
    "answer": "of the log group for the API stage.",
    "explanation": "This solution will meet the requirements by using Amazon CloudWatch Logs to capture and analyze the logs from API Gateway. Amazon CloudWatch Logs is a service that monitors, stores, and accesses log files from AWS resources. The developer can turn on execution logging and access logging in Amazon CloudWatch Logs for the API stage, which enables logging information about API execution and client access to the API. The developer can create a CloudWatch Logs log group, which is a collection of log streams that share the same retention, monitoring, and access control settings. The developer can specify the Amazon Resource Name (ARN) of the log group for the API stage, which instructs API Gateway to send the logs to the specified log group. The developer can then examine the logs to determine the cause of the HTTP 400 response errors. Option A is not optimal because it will create an Amazon Kinesis Data Firehose delivery stream to receive API call logs from API Gateway, which may introduce "
  },
  {
    "question": "A company needs to deploy all its cloud resources by using AWS CloudFormation templates A developer must create an Amazon Simple Notification Service (Amazon SNS) automatic notification to help enforce this rule. The developer creates an SNS topic and subscribes the email address of the company's security team to the SNS topic. The security team must receive a notification immediately if an 1AM role is created without the use of CloudFormation. Which solution will meet this requirement? Create an AWS Lambda function to filter events from CloudTrail if a role was created without CloudFormation Configure the Lambda",
    "options": [
      "function to publish to the SNS topi",
      "Create an Amazon EventBridge schedule to invoke the Lambda function every 15 minutes",
      "Create an AWS Fargate task in Amazon Elastic Container Service (Amazon ECS) to filter events from CloudTrail if a role was created without CloudFormation Configure the Fargate task to publish to the SNS topic Create an Amazon EventBridge schedule to run the Fargate task every 15 minutes",
      "Launch an Amazon EC2 instance that includes a script to filter events from CloudTrail if a role was created without CloudFormatio",
      "Configure the script to publish to the SNS topi. Create a cron job to run the script on the EC2 instance every 15 minutes.. Create an Amazon EventBridge rule to filter events from CloudTrail if a role was created without CloudFormation Specify the SNS topic as the target of the EventBridge rule."
    ],
    "answer": "Launch an Amazon EC2 instance that includes a script to filter events from CloudTrail if a role was created without CloudFormatio",
    "explanation": "Creating an Amazon EventBridge rule is the most efficient and scalable way to monitor and react to events from CloudTrail, such as the creation of an IAM role without CloudFormation. EventBridge allows you to specify a filter pattern to match the events you are interested in, and then specify an SNS topic as the target to send notifications. This solution does not require any additional resources or code, and it can trigger notifications in near real-time. The other solutions involve creating and managing additional resources, such as Lambda functions, Fargate tasks, or EC2 instances, and they rely on polling CloudTrail events every 15 minutes, which can introduce delays and increase costs. References ? Using Amazon EventBridge rules to process AWS CloudTrail events ? Using AWS CloudFormation to create and manage AWS Batch resources ? How to use AWS CloudFormation to configure auto scaling for Amazon Cognito and AWS AppSync ? Using AWS CloudFormation to automate the creation of AWS WAF"
  },
  {
    "question": "A developer is creating a simple proof-of-concept demo by using AWS CloudFormation and AWS Lambda functions The demo will use a CloudFormation template to deploy an existing Lambda function The Lambda function uses deployment packages and dependencies stored in Amazon S3 The developer defined anAWS Lambda Function resource in a CloudFormation template. The developer needs to add the S3 bucket to the CloudFormation template. What should the developer do to meet these requirements with the LEAST development effort?",
    "options": [
      "Add the function code in the CloudFormation template inline as the code property",
      "Add the function code in the CloudFormation template as the ZipFile property.",
      "Find the S3 key for the Lambda function Add the S3 key as the ZipFile property in the CloudFormation template.",
      "Add the relevant key and bucket to the S3Bucket and S3Key properties in the CloudFormation template"
    ],
    "answer": "Add the relevant key and bucket to the S3Bucket and S3Key properties in the CloudFormation template",
    "explanation": "The easiest way to add the S3 bucket to the CloudFormation template is to use the S3Bucket and S3Key properties of the AWS::Lambda::Function resource. These properties specify the name of the S3 bucket and the location of the .zip file that contains the function code and dependencies. This way, the developer function code or upload it to a different location. The other options are either not feasible or not efficient. does not need to modify the The code property can only be used for inline code, not for code stored in S3. The ZipFile property can only be used for code that is less than 4096 bytes, not for code that has dependencies. Finding the S3 key for the Lambda function and adding it as the ZipFile property would not work, as the ZipFile property expects a base64-encoded .zip file, not an S3 location. References ? AWS::Lambda::Function - AWS CloudFormation ? Deploying Lambda functions as .zip file archives ? AWS Lambda Function Code - AWS CloudFormation"
  },
  {
    "question": "A developer is creating an AWS Lambda function that searches for Items from an Amazon DynamoDQ table that contains customer contact information. The DynamoDB table items have the customers as the partition and additional properties such as customer -type, name, and job_title. The Lambda function runs whenever a user types a new character into the customer_type text Input. The developer wants to search to return partial matches of all tne email_address property of a particular customer type. The developer does not want to recreate the DynamoDB table. What should the developer do to meet these requirements?",
    "options": [
      "Add a global secondary index (GSI) to the DynamoDB table with customer-type input, as the partition key and email_address as the sort ke",
      "Perform a query operation on the GSI by using the begins with key condition expression with the email_address property. Add a global secondary index (GSI) to the DynamoDB table with email_address as the partition key and customer_type as the sort",
      "Perform a query operation on the GSI by using the begine_with key condition expresses with the emai"
    ],
    "answer": "Add a global secondary index (GSI) to the DynamoDB table with customer-type input, as the partition key and email_address as the sort ke",
    "explanation": "The solution that will meet the requirements is to add a global secondary index (GSI) to the DynamoDB table with customer_type as the partition key and email_address as the sort key. Perform a query operation on the GSI by using the begins_with key condition expression with the email_address property. This way, the developer can search for partial matches of the email_address property of a particular customer type without recreating the DynamoDB table. The other options either involve using a local secondary index (LSI), which requires recreating the table, or using a different partition key, which does not allow filtering by customer_type. Reference: Using Global Secondary Indexes in DynamoDB"
  },
  {
    "question": "An online sales company is developing a serverless application that runs on AWS. The application uses an AWS Lambda function that calculates order success rates and stores the data in an Amazon DynamoDB table. A developer wants an efficient way to invoke the Lambda function every 15 minutes. Which solution will meet this requirement with the LEAST development effort?",
    "options": [
      "Create an Amazon EventBridge rule that has a rate expression that will run the rule every 15 minute",
      "Add the Lambda function as the target of the EventBridge rule.",
      "Create an AWS Systems Manager document that has a script that will invoke the Lambda function on Amazon EC2. Use a Systems Manager Run Command task to run the shell script every 15 minutes.",
      "Create an AWS Step Functions state machin",
      "Configure the state machine to invoke the Lambda function execution role at a specified interval by using a Wait stat. Set the interval to 15 minutes.. Provision a small Amazon EC2 instanc. Set up a cron job that invokes the Lambda function every 15 minutes."
    ],
    "answer": "Create an Amazon EventBridge rule that has a rate expression that will run the rule every 15 minute",
    "explanation": "The best solution for this requirement is option A. Creating an Amazon EventBridge rule that has a rate expression that will run the rule every 15 minutes and adding the Lambda function as the target of the EventBridge rule is the most efficient way to invoke the Lambda function periodically. This solution does not require any additional resources or development effort, and it leverages the built-in scheduling capabilities of EventBridge1."
  },
  {
    "question": "A developer is creating an application that will store personal health information (PHI). The PHI needs to be encrypted at all times. An encrypted Amazon RDS for MySQL DB instance is storing the data. The developer wants to increase the performance of the application by caching frequently accessed data while adding the ability to sort or rank the cached datasets. Which solution will meet these requirements?",
    "options": [
      "Create an Amazon ElastiCache for Redis instanc",
      "Enable encryption of data in transit and at res",
      "Store frequently accessed data in the cache.",
      "Create an Amazon ElastiCache for Memcached instanc"
    ],
    "answer": "Create an Amazon ElastiCache for Redis instanc",
    "explanation": "Amazon ElastiCache is a service that offers fully managed in-memory data stores that are compatible with Redis or Memcached. The developer can create an ElastiCache for Redis instance and enable encryption of data in transit and at rest. This will ensure that the PHI is encrypted at all times. The developer can store frequently accessed data in the cache and use Redis features such as sorting and ranking to enhance the performance of the application."
  },
  {
    "question": "A developer is creating a new REST API by using Amazon API Gateway and AWS Lambda. The development team tests the API and validates responses for the known use cases before deploying the API to the production environment. locally. The developer wants to make the REST API available for testing by using API Gateway Which AWS Serverless Application Model Command Line Interface (AWS SAM CLI) subcommand will meet these requirements?",
    "options": [
      "Sam local invoke",
      "Sam local generate-event",
      "Sam local start-lambda",
      "Sam local start-api"
    ],
    "answer": "Sam local start-api",
    "explanation": "? The sam local start-api subcommand allows you to run your serverless application locally for quick development and testing1. It creates a local HTTP server that acts as a proxy for API Gateway and invokes your Lambda functions based on the AWS SAM template1. You can use the sam local start-api subcommand to test your REST API locally by sending HTTP requests to the local endpoint1."
  },
  {
    "question": "A developer has written the following IAM policy to provide access to an Amazon S3 bucket: Which access does the policy allow regarding the s3:GetObject and s3:PutObject actions?",
    "options": [
      "Access on all buckets except the “DOC-EXAMPLE-BUCKET” bucket EXAMPLE-BUCKET/secrets” bucket",
      "Access on all buckets that start with “DOC-EXAMPLE-BUCKET” except the “DOC-",
      "Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket along with access to all S3 actions for objects in the “DOC-EXAMPLE-BUCKET” bucket that start with “secrets”",
      "Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket except on objects that start with “secrets”"
    ],
    "answer": "Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket except on objects that start with “secrets”",
    "explanation": "The IAM policy shown in the image is a resource-based policy that grants or denies access to an S3 bucket based on certain conditions. The first statement allows access to any S3 action on any object in the “DOC-EXAMPLE-BUCKET” bucket when the request is made over HTTPS (the value of aws:SecureTransport is true). The second statement denies access to the s3:GetObject and s3:PutObject actions on any object in the “DOC-EXAMPLE-BUCKET/secrets” prefix when the request is made over HTTP (the value of aws:SecureTransport is false). Therefore, the policy allows access on all objects in the “DOC-EXAMPLE-BUCKET” bucket except on objects that start with “secrets”. Reference: Using IAM policies for Amazon S3"
  },
  {
    "question": "A developer is creating an application that will give users the ability to store photos from their cellphones in the cloud. The application needs to support tens of thousands of users. The application uses an Amazon API Gateway REST API that is integrated with AWS Lambda functions to process the photos. The application stores details about the photos in Amazon DynamoD",
    "options": [
      "Users need to create an account to access the application. In the application, users must be able to upload photos and retrieve previously uploaded photos. The photos will range in size from 300 KB to 5 MB. Which solution will meet these requirements with the LEAST operational overhead?",
      "Use Amazon Cognito user pools to manage user account",
      "Create an Amazon Cognito user pool authorizer in API Gateway to control access to the AP",
      "Use the Lambda function to store the photos and details in the DynamoDB tabl",
      "Retrieve previously uploaded photos directly from the DynamoDB table."
    ],
    "answer": "Use Amazon Cognito user pools to manage user account",
    "explanation": "Amazon Cognito user pools is a service that provides a secure user directory that scales to hundreds of millions of users. The developer can use Amazon Cognito user pools to manage user accounts and create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. The developer can use the Lambda function to store the photos in Amazon S3, which is a highly scalable, durable, and secure object storage service. The developer can store the object’s S3 key as part of the photo details in the DynamoDB table, which is a fast and flexible NoSQL database service. The developer can retrieve previously uploaded photos by querying DynamoDB for the S3 key and fetching the photos from S3. This solution will meet the requirements with the least operational overhead."
  },
  {
    "question": "A developer must use multi-factor authentication (MF",
    "options": [
      "to access data in an Amazon S3 bucket that is in another AWS account. Which AWS Security Token Service (AWS STS) API operation should the developer use with the MFA information to meet this requirement?",
      "AssumeRoleWithWebidentity",
      "GetFederationToken",
      "AssumeRoleWithSAML",
      "AssumeRole"
    ],
    "answer": "AssumeRoleWithSAML",
    "explanation": "The AssumeRole API operation returns a set of temporary security credentials that can be used to access resources in another AWS account. The developer can specify the MFA device serial number and the MFA token code in the request parameters. This option enables the developer to use MFA to access data in an S3 bucket that is in another AWS account. The other options are not relevant or effective for this scenario. References ? AssumeRole ? Requesting Temporary Security Credentials"
  },
  {
    "question": "For a deployment using AWS Code Deploy, what is the run order of the hooks for in-place deployments?",
    "options": [
      "BeforeInstall -> ApplicationStop -> ApplicationStart -> AfterInstall",
      "ApplicationStop -> BeforeInstall -> AfterInstall -> ApplicationStart",
      "BeforeInstall -> ApplicationStop -> ValidateService -> ApplicationStart",
      "ApplicationStop -> BeforeInstall -> ValidateService -> ApplicationStart"
    ],
    "answer": "ApplicationStop -> BeforeInstall -> AfterInstall -> ApplicationStart",
    "explanation": "For in-place deployments, AWS CodeDeploy uses a set of predefined hooks that run in a specific order during each deployment lifecycle event. The hooks are ApplicationStop, BeforeInstall, AfterInstall, ApplicationStart, and ValidateService. The run order of the hooks for in-place deployments is as follows: ? ApplicationStop: This hook runs first on all instances and stops the current application that is running on the instances. ? BeforeInstall: This hook runs after ApplicationStop on all instances and performs any tasks required before installing the new application revision. ? AfterInstall: This hook runs after BeforeInstall on all instances and performs any tasks required after installing the new application revision. ? ApplicationStart: This hook runs after AfterInstall on all instances and starts the new application that has been installed on the instances. ? ValidateService: This hook runs last on all instances and verifies that the new application is running properly on the insta"
  },
  {
    "question": "A developer has observed an increase in bugs in the AWS Lambda functions that a development team has deployed in its Node.js application. To minimize these bugs, the developer wants to implement automated testing of Lambda functions in an environment that closely simulates the Lambda environment. The developer needs to give other developers the ability to run the tests locally. The developer also needs to integrate the tests into the team's continuous integration and continuous delivery (CI/C",
    "options": [
      "pipeline before the AWS Cloud Development Kit (AWS CDK) deployment. Which solution will meet these requirements?",
      "Create sample events based on the Lambda documentatio",
      "Create automated test scripts that use the cdk local invoke command to invoke the Lambda function",
      "Check the respons",
      "Document the test scripts for the other developers on the tea"
    ],
    "answer": "Create automated test scripts that use the cdk local invoke command to invoke the Lambda function",
    "explanation": "The AWS Serverless Application Model Command Line Interface (AWS SAM CLI) is a command-line tool for local development and testing of Serverless applications3. The sam local generate-event command of AWS SAM CLI generates sample events for automated tests3. The sam local invoke command is used to invoke Lambda functions3. Therefore, option C is correct."
  },
  {
    "question": "A developer is testing an application that invokes an AWS Lambda function asynchronously. During the testing phase the Lambda function fails to process after two retries. How can the developer troubleshoot the failure?",
    "options": [
      "Configure AWS CloudTrail logging to investigate the invocation failures.",
      "Configure Dead Letter Queues by sending events to Amazon SQS for investigation.",
      "Configure Amazon Simple Workflow Service to process any direct unprocessed events.",
      "Configure AWS Config to process any direct unprocessed events."
    ],
    "answer": "Configure Dead Letter Queues by sending events to Amazon SQS for investigation.",
    "explanation": "This solution allows the developer to troubleshoot the failure by capturing unprocessed events in a queue for further analysis. Dead Letter Queues (DLQs) are queues that store messages that could not be processed by a service, such as Lambda, for various reasons, such as configuration errors, throttling limits, or permissions issues. The developer can configure DLQs for Lambda functions by sending events to either an Amazon Simple Queue Service (SQS) queue or an Amazon Simple Notification Service (SNS) topic. The developer can then inspect the messages in the queue or topic to identify and fix the root cause of the failure. Configuring AWS CloudTrail logging will not capture invocation failures for asynchronous Lambda invocations, but only record API calls made by or on behalf of Lambda. Configuring Amazon Simple Workflow Service (SWF) or AWS Config will not process any direct unprocessed events, but require additional integration and configuration. Reference: [Using AWS Lambda with DL"
  },
  {
    "question": "A company has an application that stores data in Amazon RDS instances. The application periodically experiences surges of high traffic that cause performance problems. During periods of peak traffic, a developer notices a reduction in query speed in all database queries. The team's technical lead determines that a multi-threaded and scalable caching solution should be used to offload the heavy read traffic. The solution needs to improve performance. Which solution will meet these requirements with the LEAST complexity?",
    "options": [
      "Use Amazon ElastiCache for Memcached to offload read requests from the main database.",
      "Replicate the data to Amazon DynamoD",
      "Set up a DynamoDB Accelerator (DAX) cluster.",
      "Configure the Amazon RDS instances to use Multi-AZ deployment with one standby instanc",
      "Offload read requests from the main database to the standby instance.. Use Amazon ElastiCache for Redis to offload read requests from the main database."
    ],
    "answer": "Use Amazon ElastiCache for Memcached to offload read requests from the main database.",
    "explanation": "? Amazon ElastiCache for Memcached is a fully managed, multithreaded, and scalable in-memory key-value store that can be used to cache frequently accessed data and improve application performance1. By using Amazon ElastiCache for Memcached, the developer can reduce the load on the main database and handle high traffic surges more efficiently. ? To use Amazon ElastiCache for Memcached, the developer needs to create a cache cluster with one or more nodes, and configure the application to store and retrieve data from the cache cluster2. The developer can use any of the supported Memcached clients to interact with the cache cluster3. The developer can also use Auto Discovery to dynamically discover and connect to all cache nodes in a cluster4. ? Amazon ElastiCache for Memcached is compatible with the Memcached protocol, which means that the developer can use existing tools and libraries that work with Memcached1. Amazon ElastiCache for Memcached also supports data partitioning, which allow"
  },
  {
    "question": "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket the developer must encrypt these objects at rest by using server- side encryption with Amazon S3 managed keys (SSE- S3). Which solution will meet this requirement?",
    "options": [
      "Create an AWS Key Management Service (AWS KMS) ke",
      "Assign the KMS key to the S3 bucket.",
      "Set the x-amz-server-side-encryption header when invoking the PutObject API operation.",
      "Provide the encryption key in the HTTP header of every request.",
      "Apply TLS to encrypt the traffic to the S3 bucket."
    ],
    "answer": "Assign the KMS key to the S3 bucket.",
    "explanation": "Amazon S3 supports server-side encryption, which encrypts data at rest on the server that stores the data. One of the encryption options is SSE-S3, which uses keys managed by S3. To use SSE-S3, the x-amz-server-side-encryption header must be set to AES256 when invoking the PutObject API operation. This instructs downloaded. Reference: S3 to encrypt the object data with SSE-S3 before saving it on disks in its data centers and decrypt it when it is Protecting data using server-side encryption with Amazon S3- managed encryption keys (SSE-S3)"
  },
  {
    "question": "A developer is working on a serverless application that needs to process any changes to an Amazon DynamoDB table with an AWS Lambda function. How should the developer configure the Lambda function to detect changes to the DynamoDB table?",
    "options": [
      "Create an Amazon Kinesis data stream, and attach it to the DynamoDB tabl",
      "Create a trigger to connect the data stream to the Lambda function. schedul",
      "Create an Amazon EventBridge rule to invoke the Lambda function on a regular",
      "Conned to the DynamoDB table from the Lambda function to detect changes.",
      "Enable DynamoDB Streams on the tabl. Create a trigger to connect the DynamoDB stream to the Lambda function.. Create an Amazon Kinesis Data Firehose delivery stream, and attach it to the DynamoDB tabl. Configure the delivery stream destination as the Lambda function."
    ],
    "answer": "Create an Amazon EventBridge rule to invoke the Lambda function on a regular",
    "explanation": "Amazon DynamoDB is a fully managed NoSQL database service that provides fast and consistent performance with seamless scalability. DynamoDB Streams is a feature that captures data modification events in DynamoDB tables. The developer can enable DynamoDB Streams on the table and create a trigger to connect the DynamoDB stream to the Lambda function. This solution will enable the Lambda function to detect changes to the DynamoDB table in near real time."
  },
  {
    "question": "A company built an online event platform For each event the company organizes quizzes and generates leaderboards that are based on the quiz scores. The company stores the leaderboard data in Amazon DynamoDB and retains the data for 30 days after an event is complete The company then uses a scheduled job to delete the old leaderboard data The DynamoDB table is configured with a fixed write capacity. During the months when many events occur, the DynamoDB write API requests are throttled when the scheduled delete job runs. A developer must create a long-term solution that deletes the old leaderboard data and optimizes write throughput Which solution meets these requirements?",
    "options": [
      "Configure a TTL attribute for the leaderboard data",
      "Use DynamoDB Streams to schedule and delete the leaderboard data",
      "Use AWS Step Functions to schedule and delete the leaderboard data.",
      "Set a higher write capacity when the scheduled delete job runs"
    ],
    "answer": "Configure a TTL attribute for the leaderboard data",
    "explanation": "\"deletes the item from your table without consuming any write throughput\" https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
  },
  {
    "question": "A developer is writing an application that will retrieve sensitive data from a third-party system. The application will format the data into a PDF file. The PDF file could be more than 1 M",
    "options": [
      "The application will encrypt the data to disk by using AWS Key Management Service (AWS KMS). The application will decrypt the file when a user requests to download it. The retrieval and formatting portions of the application are complete. The developer needs to use the GenerateDataKey API to encrypt the PDF file so that the PDF file can be decrypted later. The developer needs to use an AWS KMS symmetric customer managed key for encryption. Which solutions will meet these requirements?",
      "Write the encrypted key from the GenerateDataKey API to disk for later us plaintext key from the GenerateDataKey API and a symmetric encryption algorithm to encrypt the file.",
      "Use the",
      "Write the plain text key from the GenerateDataKey API to disk for later us",
      "Use the encrypted key from the GenerateDataKey API and a symmetric encryption algorithm to encrypt the file."
    ],
    "answer": "The application will encrypt the data to disk by using AWS Key Management Service (AWS KMS). The application will decrypt the file when a user requests to download it. The retrieval and formatting portions of the application are complete. The developer needs to use the GenerateDataKey API to encrypt the PDF file so that the PDF file can be decrypted later. The developer needs to use an AWS KMS symmetric customer managed key for encryption. Which solutions will meet these requirements?",
    "explanation": "? The GenerateDataKey API returns a data key that is encrypted under a symmetric encryption KMS key that you specify, and a plaintext copy of the same data key1. The data key is a random byte string that can be used with any standard encryption algorithm, such as AES or SM42. The plaintext data key can be used to encrypt or decrypt data outside of AWS KMS, while the encrypted data key can be stored with the encrypted data and later decrypted by AWS KMS1. ? In this scenario, the developer needs to use the GenerateDataKey API to encrypt the PDF file so that it can be decrypted later. The developer also needs to use an AWS KMS symmetric customer managed key for encryption. To achieve this, the developer can follow these steps:"
  },
  {
    "question": "An application that runs on AWS Lambda requires access to specific highly confidential objects in an Amazon S3 bucket. In accordance with the principle of least privilege a company grants access to the S3 bucket by using only temporary credentials. How can a developer configure access to the S3 bucket in the MOST secure way?",
    "options": [
      "Hardcode the credentials that are required to access the S3 objects in the application cod",
      "Use the credentials to access me required S3 objects.",
      "D. Store the key and key ID in AWS Secrets Manage"
    ],
    "answer": "D. Store the key and key ID in AWS Secrets Manage",
    "explanation": "This solution will meet the requirements by creating a Lambda function execution role, which is an IAM role that grants permissions to a Lambda function to access AWS resources such as Amazon S3 objects. The developer can attach a policy to the role that grants access to specific objects in the S3 bucket that are required by the application, following the principle of least privilege. Option A is not optimal because it will hardcode the credentials that are required to access S3 objects in the application code, which is insecure and difficult to maintain. Option B is not optimal because it will create a secret access key and access key ID with permission to access the S3 bucket, which will introduce additional security risks and complexity for storing and managing credentials. Option D is not optimal because it will store the secret access key and access key ID as environment variables in Lambda, which is also insecure and difficult to maintain."
  },
  {
    "question": "A developer has created an AWS Lambda function that makes queries to an Amazon Aurora MySQL DB instance. When the developer performs a test the OB instance shows an error for too many connections. Which solution will meet these requirements with the LEAST operational effort?",
    "options": [
      "Create a read replica for the DB instance Query the replica DB instance instead of the primary DB instance.",
      "Migrate the data lo an Amazon DynamoDB database.",
      "Configure the Amazon Aurora MySQL DB instance tor Multi-AZ deployment.",
      "Create a proxy in Amazon RDS Proxy Query the proxy instead of the DB instance."
    ],
    "answer": "Create a proxy in Amazon RDS Proxy Query the proxy instead of the DB instance.",
    "explanation": "This solution will meet the requirements by using Amazon RDS Proxy, which is a fully managed, highly available database proxy for Amazon RDS that makes applications more scalable, more resilient to database failures, and more secure. The developer can create a proxy in Amazon RDS Proxy, which sits between the application and the DB instance and handles connection management, pooling, and routing. The developer can query the proxy instead of the DB instance, which reduces the number of open connections to the DB instance and avoids errors for too many connections. Option A is not optimal because it will create a read replica for the DB instance, which may not solve the problem of too many connections as read replicas also have connection limits and may incur additional costs. Option B is not optimal because it will migrate the data to an Amazon DynamoDB database, which may introduce additional complexity and overhead for migrating and accessing data from a different database service. Op"
  },
  {
    "question": "A developer has an application that is composed of many different AWS Lambda functions. The Lambda functions all use some of the same dependencies. To avoid security issues the developer is constantly updating the dependencies of all of the Lambda functions. The result is duplicated effort to reach function. How can the developer keep the dependencies of the Lambda functions up to date with the LEAST additional complexity?",
    "options": [
      "Define a maintenance window for the Lambda functions to ensure that the functions get updated copies of the dependencies.",
      "Upgrade the Lambda functions to the most recent runtime version.",
      "Define a Lambda layer that contains all of the shared dependencies.",
      "Use an AWS CodeCommit repository to host the dependencies in a centralized location."
    ],
    "answer": "Define a Lambda layer that contains all of the shared dependencies.",
    "explanation": "This solution allows the developer to keep the dependencies of the Lambda functions up to date with the least additional complexity because it eliminates the need to update each function individually. A Lambda layer is a ZIP archive that contains libraries, custom runtimes, or other dependencies. The developer can create a layer that contains all of the shared dependencies and attach it to multiple Lambda functions. When the developer updates the layer, all of the functions that use the layer will have access to the latest version of the dependencies. Reference: [AWS Lambda layers]"
  },
  {
    "question": "A company's developer has deployed an application in AWS by using AWS CloudFormation The CloudFormation stack includes parameters in AWS Systems Manager Parameter Store that the application uses as configuration settings. The application can modify the parameter values When the developer updated the stack to create additional resources with tags, the developer noted that the parameter values were reset and that the values ignored the latest changes made by the application. The developer needs to change the way the company deploys the CloudFormation stack. The developer also needs to avoid resetting the parameter values outside the stack. Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      "Modify the CloudFormation stack to set the deletion policy to Retain for the Parameter Store parameters.",
      "Create an Amazon DynamoDB table as a resource in the CloudFormation stack to hold configuration data for the application Migrate the parameters that the application is modifying from Parameter Store to the DynamoDB table",
      "Create an Amazon RDS DB instance as a resource in the CloudFormation stac",
      "Create a table in the database for parameter configuratio",
      "Migrate the parameters that the application is modifying from Parameter Store to the configuration table. Modify the CloudFormation stack policy to deny updates on Parameter Store parameters"
    ],
    "answer": "Create a table in the database for parameter configuratio",
    "explanation": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack- resources.html#stack-policy-samples"
  },
  {
    "question": "A company needs to set up secure database credentials for all its AWS Cloud resources. The company's resources include Amazon RDS DB instances Amazon DocumentDB clusters and Amazon Aurora DB instances. The company's security policy mandates that database credentials be encrypted at rest and rotated at a regular interval. Which solution will meet these requirements MOST securely?",
    "options": [
      "Set up IAM database authentication for token-based acces",
      "Generate user tokens to provide centralized access to RDS DB instance",
      "Amazon DocumentDB clusters and Aurora DB instances.",
      "Create parameters for the database credentials in AWS Systems Manager Parameter Store Set the Type parameter to Secure Stin"
    ],
    "answer": "Create parameters for the database credentials in AWS Systems Manager Parameter Store Set the Type parameter to Secure Stin",
    "explanation": "This solution will meet the requirements by using AWS Secrets Manager, which is a service that helps protect secrets such as database credentials by encrypting them with AWS Key Management Service (AWS KMS) and enabling automatic rotation of secrets. The developer can create an AWS Lambda function by using the SecretsManagerRotationTemplate template in the AWS Secrets Manager console, which provides a sample code for rotating secrets for RDS DB instances, Amazon DocumentDB clusters, and Amazon Aurora DB instances. The developer can also create secrets for the database credentials in Secrets Manager, which encrypts them at rest and provides secure access to them. The developer can set up secrets rotation on a schedule, which changes the database credentials periodically according to a specified interval or event. Option A is not optimal because it will set up IAM database authentication for token-based access, which may not be compatible with all database engines and may require additio"
  },
  {
    "question": "A company has deployed infrastructure on AWS. A development team wants to create an AWS Lambda function that will retrieve data from an Amazon Aurora database. The Amazon Aurora database is in a private subnet in company's VP",
    "options": [
      "The VPC is named VPC1. The data is relational in nature. The Lambda function needs to access the data securely. Which solution will meet these requirements?",
      "Create the Lambda functio",
      "Configure VPC1 access for the functio",
      "Attach a security group named SG1 to both the Lambda function and the databas",
      "Configure the security group inbound and outbound rules to allow TCP traffic on Port 3306."
    ],
    "answer": "The VPC is named VPC1. The data is relational in nature. The Lambda function needs to access the data securely. Which solution will meet these requirements?",
    "explanation": "AWS Lambda is a service that lets you run code without provisioning or managing servers. Lambda functions can be configured to access resources in a VPC, such as an Aurora database, by specifying one or more subnets and security groups in the VPC settings of the function. A security group acts as a virtual firewall that controls inbound and outbound traffic for the resources in a VPC. To allow a Lambda function to communicate with an Aurora database, both resources need to be associated with the same security group, and the security group rules need to allow TCP traffic on Port 3306, which is the default port for MySQL databases. Reference: [Configuring a Lambda function to access resources in a VPC]"
  },
  {
    "question": "A company is running a custom application on a set of on-premises Linux servers that are accessed using Amazon API Gateway. AWS X-Ray tracing has been enabled on the API test stage. How can a developer enable X-Ray tracing on the on-premises servers with the LEAST amount of configuration?",
    "options": [
      "Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service.",
      "Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service.",
      "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTraceSegments API call.",
      "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTelemetryRecords API call."
    ],
    "answer": "Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service.",
    "explanation": "The X-Ray daemon is a software that collects trace data from the X-Ray SDK and relays it to the X-Ray service. The X-Ray daemon can run on any platform that supports Go, including Linux, Windows, and macOS. The developer can install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service with minimal configuration. The X-Ray SDK is used to instrument the application code, not to capture and relay data. The Lambda function solutions are more complex and require additional configuration."
  },
  {
    "question": "A company is expanding the compatibility of its photo-snaring mobile app to hundreds of additional devices with unique screen dimensions and resolutions. Photos are stored in Amazon S3 in their original format and resolution. The company uses an Amazon CloudFront distribution to serve the photos The app includes the dimension and resolution of the display as GET parameters with every request. A developer needs to implement a solution that optimizes the photos that are served to each device to reduce load time and increase photo quality. Which solution will meet these requirements MOST cost-effective?",
    "options": [
      "Use S3 Batch Operations to invoke an AWS Lambda function to create new variants of the photos with the required dimensions and resolution",
      "Create a dynamic CloudFront origin that automatically maps the request of each device to the corresponding photo variant.",
      "Use S3 Batch Operations to invoke an AWS Lambda function to create new variants of the photos with the required dimensions and resolution",
      "Create a Lambda@Edge function to route requests to the corresponding photo vacant by using request headers.",
      "Create a Lambda@Edge function that optimizes the photos upon request and returns the photos as a respons. Change the CloudFront TTL cache policy to the maximum value possible. Create a Lambda@Edge function that optimizes the photos upon request and returns the photos as a respons.. In the same function store a copy of the processed photos on Amazon S3 for subsequent requests."
    ],
    "answer": "Create a Lambda@Edge function to route requests to the corresponding photo vacant by using request headers.",
    "explanation": "This solution meets the requirements most cost-effectively because it optimizes the photos on demand and caches them for future requests. Lambda@Edge allows the developer to run Lambda functions at AWS locations closer to viewers, which can reduce latency and improve photo quality. The developer can create a Lambda@Edge function that uses the GET parameters from each request to optimize the photos with the required dimensions and resolutions and returns them as a response. The function can also store a copy of the processed photos on Amazon S3 for subsequent requests, which can reduce processing time and costs. Using S3 Batch Operations to create new variants of the photos will incur additional storage costs and may not cover all possible dimensions and resolutions. Creating a dynamic CloudFront origin or a Lambda@Edge function to route requests to corresponding photo variants will require maintaining a mapping of device types and photo variants, which can be complex and error-prone. R"
  },
  {
    "question": "A developer is creating an Amazon DynamoDB table by using the AWS CLI The DynamoDB table must use server-side encryption with an AWS owned encryption key How should the developer create the DynamoDB table to meet these requirements?",
    "options": [
      "Create an AWS Key Management Service (AWS KMS) customer managed ke",
      "Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyld parameter during creation of the DynamoDB table",
      "Create an AWS Key Management Service (AWS KMS) AWS managed key Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyld parameter during creation of the DynamoDB table",
      "Create an AWS owned key Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyld parameter during creation of the DynamoDB table.",
      "Create the DynamoDB table with the default encryption options"
    ],
    "answer": "Create an AWS owned key Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyld parameter during creation of the DynamoDB table.",
    "explanation": "When creating an Amazon DynamoDB table using the AWS CLI, server-side encryption with an AWS owned encryption key is enabled by default. Therefore, the developer does not need to create an AWS KMS key or specify the KMSMasterKeyId parameter. Option A and B are incorrect because they suggest creating customer- managed and AWS-managed KMS keys, which are not needed in this scenario. Option C is also incorrect because AWS owned keys are automatically used for server-side encryption by default."
  },
  {
    "question": "A developer is working on an ecommerce website The developer wants to review server logs without logging in to each of the application servers individually. The website runs on multiple Amazon EC2 instances, is written in Python, and needs to be highly available How can the developer update the application to meet these requirements with MINIMUM changes?",
    "options": [
      "Rewrite the application to be cloud native and to run on AWS Lambda, where the logs can be reviewed in Amazon CloudWatch",
      "Set up centralized logging by using Amazon OpenSearch Service, Logstash, and OpenSearch Dashboards Scale down the application to one larger EC2 instance where only one instance is recording logs",
      "D. Install the unified Amazon CloudWatch agent on the EC2 instances Configure the agent to push the application logs to CloudWatch"
    ],
    "answer": "Rewrite the application to be cloud native and to run on AWS Lambda, where the logs can be reviewed in Amazon CloudWatch",
    "explanation": "The unified Amazon CloudWatch agent can collect both system metrics and log files from Amazon EC2 instances and on-premises servers. By installing and configuring the agent on the EC2 instances, the developer can easily access and analyze the application logs in CloudWatch without logging in to each server individually. This option requires minimum changes to the existing application and does not affect its availability or scalability. References ? Using the CloudWatch Agent ? Collecting Metrics and Logs from Amazon EC2 Instances and On-Premises Servers with the CloudWatch Agent"
  },
  {
    "question": "An application is using Amazon Cognito user pools and identity pools for secure access. A developer wants to integrate the user-specific file upload and download features in the application with Amazon S3. The developer must ensure that the files are saved and retrieved in a secure manner and that users can access only their own files. The file sizes range from 3 KB to 300 M",
    "options": [
      "Which option will meet these requirements with the HIGHEST level of security?",
      "Use S3 Event Notifications to validate the file upload and download requests and update the user interface (UI).",
      "Save the details of the uploaded files in a separate Amazon DynamoDB tabl",
      "Filter the list of files in the user interface (UI) by comparing the current user ID with the user ID associated with the file in the table.",
      "Use Amazon API Gateway and an AWS Lambda function to upload and download file"
    ],
    "answer": "Filter the list of files in the user interface (UI) by comparing the current user ID with the user ID associated with the file in the table.",
    "explanation": "https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html"
  },
  {
    "question": "A company is planning to use AWS CodeDeploy to deploy an application to Amazon Elastic Container Service (Amazon ECS) During the deployment of a new version of the application, the company initially must expose only 10% of live traffic to the new version of the deployed application. Then, after 15 minutes elapse, the company must route all the remaining live traffic to the new version of the deployed application. Which CodeDeploy predefined configuration will meet these requirements?",
    "options": [
      "CodeDeployDefault ECSCanary10Percent15Minutes",
      "CodeDeployDefault LambdaCanary10Percent5Minutes",
      "CodeDeployDefault LambdaCanary10Percent15Minutes",
      "CodeDeployDefault ECSLinear10PercentEvery1 Minutes"
    ],
    "answer": "CodeDeployDefault ECSCanary10Percent15Minutes",
    "explanation": "The predefined configuration \"CodeDeployDefault.ECSCanary10Percent15Minutes\" is designed for Amazon Elastic Container Service (Amazon ECS) deployments and meets the specified requirements. It will perform a canary deployment, which means it will initially route 10% of live traffic to the new version of the application, and then after 15 minutes elapse, it will automatically route all the remaining live traffic to the new version. This gradual deployment approach allows the company to verify the health and performance of the new version with a small portion of traffic before fully deploying it to all users."
  },
  {
    "question": "A developer is creating an AWS Lambda function. The Lambda function needs an external library to connect to a third-party solution The external library is a collection of files with a total size of 100 MB The developer needs to make the external library available to the Lambda execution environment and reduce the Lambda package space Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "Create a Lambda layer to store the external library Configure the Lambda function to use the layer",
      "Create an Amazon S3 bucket Upload the external library into the S3 bucke",
      "Mount the S3 bucket folder in the Lambda function Import the library by using the proper folder in the mount point.",
      "Load the external library to the Lambda function's /tmp directory during deployment of the Lambda packag",
      "Import the library from the /tmp directory.. Create an Amazon Elastic File System (Amazon EFS) volum. Upload the external library to the EFS volume Mount the EFS volume in the Lambda function. Import the library by using the proper folder in the mount point."
    ],
    "answer": "Create a Lambda layer to store the external library Configure the Lambda function to use the layer",
    "explanation": "Create a Lambda layer to store the external library. Configure the Lambda function to use the layer. This will allow the developer to make the external library available to the Lambda execution environment without having to include it in the Lambda package, which will reduce the Lambda package space. Using a Lambda layer is a simple and straightforward solution that requires minimal operational overhead. https://docs.aws.amazon.com/lambda/latest/dg/configuration- layers.html"
  },
  {
    "question": "A developer is creating a template that uses AWS CloudFormation to deploy an application. The application is serverless and uses Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. Which AWS service or tool should the developer use to define serverless resources in YAML?",
    "options": [
      "CloudFormation serverless intrinsic functions",
      "AWS Elastic Beanstalk",
      "AWS Serverless Application Model (AWS SAM)",
      "AWS Cloud Development Kit (AWS CDK)"
    ],
    "answer": "AWS Serverless Application Model (AWS SAM)",
    "explanation": "AWS Serverless Application Model (AWS SAM) is an open-source framework that enables developers to build and deploy serverless applications on AWS. AWS SAM uses a template specification that extends AWS CloudFormation to simplify the definition of serverless resources such as API Gateway, DynamoDB, and Lambda. The developer can use AWS SAM to define serverless resources in YAML and deploy them using the AWS SAM CLI."
  },
  {
    "question": "A company hosts its application on AWS. The application runs on an Amazon Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. The cluster runs behind an Application Load Balancer The application stores data in an Amazon Aurora database A developer encrypts and manages database credentials inside the application The company wants to use a more secure credential storage method and implement periodic credential rotation. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "Migrate the secret credentials to Amazon RDS parameter group",
      "Encrypt the parameter by using an AWS Key Management Service (AWS KMS) key Turn on secret rotatio",
      "Use 1AM policies and roles to grant AWS KMS permissions to access Amazon RDS.",
      "Migrate the credentials to AWS Systems Manager Parameter Stor"
    ],
    "answer": "Migrate the credentials to AWS Systems Manager Parameter Stor",
    "explanation": "AWS Secrets Manager is a service that helps you store, distribute, and rotate secrets securely. You can use Secrets Manager to migrate your credentials from your application code to a secure and encrypted storage. You can also enable automatic rotation of your secrets by using AWS Lambda functions or custom logic. You can use IAM policies and roles to grant your Amazon ECS Fargate tasks permissions to access your secrets from Secrets Manager. This solution minimizes the operational overhead of managing your credentials and enhances the security of your application. References ? AWS Secrets Manager: Store, Distribute, and Rotate Credentials Securely | AWS News Blog ? Why You Should Audit and Rotate Your AWS Credentials Periodically - Cloud Academy ? Top 5 AWS root account best practices - TheServerSide"
  },
  {
    "question": "A company has an Amazon S3 bucket that contains sensitive data. The data must be encrypted in transit and at rest. The company encrypts the data in the S3 bucket by using an AWS Key Management Service (AWS KMS) key. A developer needs to grant several other AWS accounts the permission to use the S3 GetObject operation to retrieve the data from the S3 bucket. How can the developer enforce that all requests to retrieve the data provide encryption in transit?",
    "options": [
      "Define a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false”.",
      "Define a resource-based policy on the S3 bucket to allow access when a request meets the condition “aws:SecureTransport”: “false”.",
      "Define a role-based policy on the other accounts' roles to deny access when a request meets the condition of “aws:SecureTransport”: “false”.",
      "Define a resource-based policy on the KMS key to deny access when a request meets the condition of “aws:SecureTransport”: “false”."
    ],
    "answer": "Define a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false”.",
    "explanation": "Amazon S3 supports resource-based policies, which are JSON documents that specify the permissions for accessing S3 resources. A resource-based policy can be used to enforce encryption in transit by denying access to requests that do not use HTTPS. The condition key aws:SecureTransport can be used to check if the request was sent using SSL. If the value of this key is false, the request is denied; otherwise, the request is allowed. Reference: How do I use an S3 bucket policy to require requests to use Secure Socket Layer (SSL)?"
  },
  {
    "question": "A company is using Amazon API Gateway to invoke a new AWS Lambda function The company has Lambda function versions in its PROD and DEV environments. In each environment, there is a Lambda function alias pointing to the corresponding Lambda function version API Gateway has one stage that is configured to point at the PROD alias The company wants to configure API Gateway to enable the PROD and DEV Lambda function versions to be simultaneously and distinctly available Which solution will meet these requirements?",
    "options": [
      "Enable a Lambda authorizer for the Lambda function alias in API Gateway Republish PROD and create a new stage for DEV Create API Gateway stage variables for the PROD and DEV stage",
      "Point each stage variable to the PROD Lambda authorizer to the DEV Lambda authorizer.",
      "Set up a gateway response in API Gateway for the Lambda function alia",
      "Republish PROD and create a new stage for DE"
    ],
    "answer": "Republish PROD and create a new stage for DE",
    "explanation": "The best solution is to use an API Gateway stage variable to configure the Lambda function alias. This allows you to specify the Lambda function name and its alias or version using the syntax function_name:$ {stageVariables.variable_name} in the Integration Request. You can then create different stages in API Gateway, such as PROD and DEV, and assign different values to the stage variable for each stage. This way, you can invoke different Lambda function versions or aliases based on the stage that you are using, without changing the function name in the Integration Request. References ? Using API Gateway stage variables to manage Lambda functions ? How to point AWS API gateway stage to specific lambda function alias? ? Setting stage variables using the Amazon API Gateway console ? Amazon API Gateway stage variables reference"
  },
  {
    "question": "A developer is writing a serverless application that requires an AWS Lambda function to be invoked every 10 minutes. What is an automated and serverless way to invoke the function?",
    "options": [
      "Deploy an Amazon EC2 instance based on Linux, and edit its /etc/confab file by adding a command to periodically invoke the lambda function",
      "Configure an environment variable named PERIOD for the Lambda functio",
      "Set the value to 600.",
      "Create an Amazon EventBridge rule that runs on a regular schedule to invoke the Lambda function.",
      "Create an Amazon Simple Notification Service (Amazon SNS) topic that has a subscription to the Lambda function with a 600-second timer."
    ],
    "answer": "Set the value to 600.",
    "explanation": "The solution that will meet the requirements is to create an Amazon EventBridge rule that runs on a regular schedule to invoke the Lambda function. This way, the developer can use an automated and serverless way to invoke the function every 10 minutes. The developer can also use a cron expression or a rate expression to specify the schedule for the rule. The other options either involve using an Amazon EC2 instance, which is not serverless, or using environment variables or query parameters, which do not trigger the function. Reference: Schedule AWS Lambda functions using EventBridge"
  },
  {
    "question": "A developer is building a serverless application that is based on AWS Lambda. The developer initializes the AWS software development kit (SDK) outside of the Lambda handcar function. What is the PRIMARY benefit of this action?",
    "options": [
      "Improves legibility and systolic convention",
      "Takes advantage of runtime environment reuse",
      "Provides better error handling",
      "Creates a new SDK instance for each invocation"
    ],
    "answer": "Takes advantage of runtime environment reuse",
    "explanation": "This benefit occurs when initializing the AWS SDK outside of the Lambda handler function because it allows the SDK instance to be reused across multiple invocations of the same function. This can improve performance and reduce latency by avoiding unnecessary initialization overhead. If the SDK is initialized inside the handler function, it will create a new SDK instance for each invocation, which can increase memory usage and execution time. Reference: [AWS Lambda execution environment], [Best Practices for Working with AWS Lambda Functions]"
  },
  {
    "question": "A developer wants to insert a record into an Amazon DynamoDB table as soon as a new file is added to an Amazon S3 bucket. Which set of steps would be necessary to achieve this?",
    "options": [
      "Create an event with Amazon EventBridge that will monitor the S3 bucket and then insert the records into DynamoDB.",
      "Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.",
      "Create an AWS Lambda function that will poll the S3 bucket and then insert the records into DynamoDB.",
      "Create a cron job that will run at a scheduled time and insert the records into DynamoDB."
    ],
    "answer": "Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.",
    "explanation": "Amazon S3 is a service that provides highly scalable, durable, and secure object storage. Amazon DynamoDB is a fully managed NoSQL database service that provides fast and consistent performance with seamless scalability. AWS Lambda is a service that lets developers run code without provisioning or managing servers. The developer can configure an S3 event to invoke a Lambda function that inserts records into DynamoDB whenever a new file is added to the S3 bucket. This solution will meet the requirement of inserting a record into DynamoDB as soon as a new file is added to S3."
  },
  {
    "question": "A developer is designing an AWS Lambda function that creates temporary files that are less than 10 MB during invocation. The temporary files will be accessed and modified multiple times during invocation. The developer has no need to save or retrieve these files in the future. Where should the temporary files be stored?",
    "options": [
      "the /tmp directory",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon S3"
    ],
    "answer": "the /tmp directory",
    "explanation": "AWS Lambda is a service that lets developers run code without provisioning or managing servers. Lambda provides a local file system that can be used to store temporary files during invocation. The local file system is mounted under the /tmp directory and has a limit of 512 MB. The temporary files are accessible only by the Lambda function that created them and are deleted after the function execution ends. The developer can store temporary files that are less than 10 MB in the /tmp directory and access and modify them multiple times during invocation."
  },
  {
    "question": "A developer has written an AWS Lambda function. The function is CPU-bound. The developer wants to ensure that the function returns responses quickly. How can the developer improve the function's performance?",
    "options": [
      "Increase the function's CPU core count.",
      "Increase the function's memory.",
      "Increase the function's reserved concurrency.",
      "Increase the function's timeout."
    ],
    "answer": "Increase the function's memory.",
    "explanation": "The amount of memory you allocate to your Lambda function also determines how much CPU and network bandwidth it gets. Increasing the memory size can improve the performance of CPU-bound functions by giving them more CPU power. The CPU allocation is proportional to the memory allocation, so a function with 1 GB of memory has twice the CPU power of a function with 512 MB of memory. Reference: AWS Lambda execution environment"
  },
  {
    "question": "A developer must analyze performance issues with production-distributed applications written as AWS Lambda functions. These distributed Lambda applications invoke other components that make up me applications. How should the developer identify and troubleshoot the root cause of the performance issues in production?",
    "options": [
      "Add logging statements to the Lambda function",
      "then use Amazon CloudWatch to view the logs.",
      "Use AWS CloudTrail and then examine the logs.",
      "Use AWS X-Ra",
      "then examine the segments and errors.. Run Amazon inspector agents and then analyze performance."
    ],
    "answer": "Use AWS CloudTrail and then examine the logs.",
    "explanation": "This solution will meet the requirements by using AWS X-Ray to analyze and debug the performance issues with the distributed Lambda applications. AWS X-Ray is a service that collects data about requests that the applications serve, and provides tools to view, filter, and gain insights into that data. The developer can use AWS X-Ray to identify the root cause of the performance issues by examining the segments and errors that show the details of each request and the components that make up the applications. Option A is not optimal because it will use logging statements and Amazon CloudWatch, which may not provide enough information or visibility into the distributed applications. Option B is not optimal because it will use AWS CloudTrail, which is a service that records API calls and events for AWS services, not application performance data. Option D is not optimal because it will use Amazon Inspector, which is a service that helps improve the security and compliance of applications on "
  },
  {
    "question": "A company is implementing an application on Amazon EC2 instances. The application needs to process incoming transactions. When the application detects a transaction that is not valid, the application must send a chat message to the company's support team. To send the message, the application needs to retrieve the access token to authenticate by using the chat API. A developer needs to implement a solution to store the access token. The access token must be encrypted at rest and in transit. The access token must also be accessible from other AWS accounts. Which solution will meet these requirements with the LEAST management overhead?",
    "options": [
      "Use an AWS Systems Manager Parameter Store SecureString parameter that uses an AWS Key Management Service (AWS KMS) AWS managed key to store the access toke",
      "Add a resource-based policy to the parameter to allow access from other account",
      "Update the IAM role of the EC2 instances with permissions to access Parameter Stor the token from Parameter Store with the decrypt flag enable",
      "Retrieve"
    ],
    "answer": "Update the IAM role of the EC2 instances with permissions to access Parameter Stor the token from Parameter Store with the decrypt flag enable",
    "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/secrets-manager-share-between-accounts/ https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and- access_examples_cross.html"
  },
  {
    "question": "A developer accesses AWS CodeCommit over SSH. The SSH keys configured to access AWS CodeCommit are tied to a user with the following permissions: The developer needs to create/delete branches Which specific IAM permissions need to be added based on the principle of least privilege?",
    "options": [
      "Option A",
      "Option B",
      "Option C",
      "Option D"
    ],
    "answer": "Option A",
    "explanation": "This solution allows the developer to create and delete branches in AWS CodeCommit by granting the codecommit:CreateBranch and codecommit:DeleteBranch permissions. These are the minimum permissions required for this task, following the principle of least privilege. Option B grants too many permissions, such codecommit:Put*, which allows the developer to create, update, or delete any resource in CodeCommit. Option C grants too few as permissions, such as codecommit:Update*, which does not allow the developer to create or delete branches. Option D grants all permissions, such as codecommit:*, which is not secure or recommended. Reference: [AWS CodeCommit Permissions Reference], [Create a Branch (AWS CLI)]"
  },
  {
    "question": "A developer needs to store configuration variables for an application. The developer needs to set an expiration date and time for me configuration. The developer wants to receive notifications. Before the configuration expires. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "Create a standard parameter in AWS Systems Manager Parameter Store Set Expiation and Expiration Notification policy types.",
      "Create a standard parameter in AWS Systems Manager Parameter Store Create an AWS Lambda function to expire the configuration and to send Amazon Simple Notification Service (Amazon SNS) notifications.",
      "Create an advanced parameter in AWS Systems Manager Parameter Store Set Expiration and Expiration Notification policy types.",
      "Create an advanced parameter in AWS Systems Manager Parameter Store Create an Amazon EC2 instance with a corn job to expire the configuration and to send notifications."
    ],
    "answer": "Create an advanced parameter in AWS Systems Manager Parameter Store Set Expiration and Expiration Notification policy types.",
    "explanation": "This solution will meet the requirements by creating an advanced parameter in AWS Systems Manager Parameter Store, which is a secure and scalable service for storing and managing configuration data and secrets. The advanced parameter allows setting expiration and expiration notification policy types, which enable specifying an expiration date and time for the configuration and receiving notifications before the configuration expires. The Lambda code will be refactored to load the Root CA Cert from the parameter store and modify the runtime trust store outside the Lambda function handler, which will improve performance and reduce latency by avoiding repeated calls to Parameter Store and trust store modifications for each invocation of the Lambda function. Option A is not optimal because it will create a standard parameter in AWS Systems Manager Parameter Store, which does not support expiration and expiration notification policy types. Option B is not optimal because it will create a se"
  },
  {
    "question": "A developer wants to deploy a new version of an AWS Elastic Beanstalk application. During deployment the application must maintain full capacity and avoid service interruption. Additionally, the developer must minimize the cost of additional resources that support the deployment. Which deployment method should the developer use to meet these requirements?",
    "options": [
      "All at once",
      "Rolling with additional batch",
      "Bluegreen",
      "Immutable"
    ],
    "answer": "Rolling with additional batch",
    "explanation": "This solution will meet the requirements by using a rolling with additional batch deployment method, which deploys the new version of the application to a separate group of instances and then shifts traffic to those instances in batches. This way, the application maintains full capacity and avoids service interruption during deployment, as well as minimizes the cost of additional resources that support the deployment. Option A is not optimal because it will use an all at once deployment method, which deploys the new version of the application to all instances simultaneously, which may cause service interruption or downtime during deployment. Option C is not optimal because it will use a blue/green deployment method, which deploys the new version of the application to a separate environment and then swaps URLs with the original environment, which may incur more costs for additional resources that support the deployment. Option D is not optimal because it will use an immutable deployment"
  },
  {
    "question": "A developer is troubleshooting an application mat uses Amazon DynamoDB in the uswest- 2 Region. The application is deployed to an Amazon EC2 instance. The application requires read-only permissions to a table that is named Cars The EC2 instance has an attached IAM role that contains the following IAM policy. When the application tries to read from the Cars table, an Access Denied error occurs. How can the developer resolve this error?",
    "options": [
      "Modify the IAM policy resource to be \"arn aws dynamo* us-west-2 account-id table/*\"",
      "Modify the IAM policy to include the dynamodb * action",
      "Create a trust policy that specifies the EC2 service principa",
      "Associate the role with the policy.",
      "Create a trust relationship between the role and dynamodb Amazonas com."
    ],
    "answer": "Create a trust policy that specifies the EC2 service principa",
    "explanation": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/access-control- overview.html#access-control-resource-ownership"
  },
  {
    "question": "A developer is preparing to begin development of a new version of an application. The previous version of the application is deployed in a production environment. The developer needs to deploy fixes and updates to the current version during the development of the new version of the application. The code for the new version of the application is stored in AWS CodeCommit. Which solution will meet these requirements?",
    "options": [
      "From the main branch, create a feature branch for production bug fixe",
      "Create a second feature branch from the main branch for development of the new version.",
      "Create a Git tag of the code that is currently deployed in productio",
      "Create a Git tag for the development of the new versio",
      "Push the two tags to the CodeCommit repository.. From the main branch, create a branch of the code that is currently deployed in productio. Apply an IAM policy that ensures no other other users can push or merge to the branch."
    ],
    "answer": "From the main branch, create a feature branch for production bug fixe",
    "explanation": "? A feature branch is a branch that is created from the main branch to work on a specific feature or task1. Feature branches allow developers to isolate their work from the main branch and avoid conflicts with other changes1. Feature branches can be merged back to the main branch when the feature or task is completed and tested1. ? In this scenario, the developer needs to maintain two parallel streams of work: one for fixing and updating the current version of the application that is deployed in production, and another for developing the new version of the application. The developer can use feature branches to achieve this goal. ? The developer can create a feature branch from the main branch for production bug fixes. This branch will contain the code that is currently deployed in production, and any fixes or updates that need to be applied to it. The developer can push this branch to the CodeCommit repository and use it to deploy changes to the production environment. ? The developer "
  },
  {
    "question": "A developer is creating a service that uses an Amazon S3 bucket for image uploads. The service will use an AWS Lambda function to create a thumbnail of each image Each time an image is uploaded the service needs to send an email notification and create the thumbnail The developer needs to configure the image setup. processing and email notifications Which solution will meet these requirements?",
    "options": [
      "Create an Amazon Simple Notification Service (Amazon SNS) topic Configure S3 event notifications with a destination of the SNS topic Subscribe the Lambda function to the SNS topic Create an email notification subscription to the SNS topic",
      "Create an Amazon Simple Notification Service (Amazon SNS) topi",
      "Configure S3 event notifications with a destination of the SNS topi",
      "Subscribe the Lambda function to the SNS topi"
    ],
    "answer": "Create an Amazon Simple Notification Service (Amazon SNS) topic Configure S3 event notifications with a destination of the SNS topic Subscribe the Lambda function to the SNS topic Create an email notification subscription to the SNS topic",
    "explanation": "This solution will allow the developer to receive notifications for each image uploaded to the S3 bucket, and also create a thumbnail using the Lambda function. The SNS topic will serve as a trigger for both the Lambda function and the email notification subscription. When an image is uploaded, S3 will send a notification to the SNS topic, which will trigger the Lambda function to create the thumbnail and also send an email notification to the specified email address."
  },
  {
    "question": "A developer needs to perform geographic load testing of an API. The developer must deploy resources to multiple AWS Regions to support the load testing of the API. How can the developer meet these requirements without additional application code?",
    "options": [
      "Create and deploy an AWS Lambda function in each desired Regio",
      "Configure the Lambda function to create a stack from an AWS CloudFormation template in that Region when the function is invoked. Create an AWS CloudFormation template that defines the load test resource",
      "D. Use the AWS CLI create-stack-set command to create a stack set in the desired Regions.",
      "Create an AWS Systems Manager document that defines the resource. Use the document to create the resources in the desired Regions.. Create an AWS CloudFormation template that defines the load test resource. Use the AWS CLI deploy command to create a stack from the template in each Region."
    ],
    "answer": "Configure the Lambda function to create a stack from an AWS CloudFormation template in that Region when the function is invoked. Create an AWS CloudFormation template that defines the load test resource",
    "explanation": "AWS CloudFormation is a service that allows developers to model and provision AWS resources using templates. A CloudFormation template can define the load test resources, such as EC2 instances, load balancers, and Auto Scaling groups. A CloudFormation stack set is a collection of stacks that can be created and managed from a single template in multiple Regions and accounts. The AWS CLI create-stack-set command can be used to create a stack set from a template and specify the Regions where the stacks should be created. Reference: Working with AWS CloudFormation stack sets"
  },
  {
    "question": "A developer is building an application that uses AWS API Gateway APIs. AWS Lambda function, and AWS Dynamic DB tables. The developer uses the AWS Serverless Application Model (AWS SAM) to build and run serverless applications on AWS. Each time the developer pushes of changes for only to the Lambda functions, all the artifacts in the application are rebuilt. The developer wants to implement AWS SAM Accelerate by running a command to only redeploy the Lambda functions that have changed. Which command will meet these requirements?",
    "options": [
      "sam deploy -force-upload",
      "sam deploy -no-execute-changeset",
      "sam package",
      "sam sync -watch"
    ],
    "answer": "sam sync -watch",
    "explanation": "The command that will meet the requirements is sam sync -watch. This command enables AWS SAM Accelerate mode, which allows the developer to only redeploy the Lambda functions that have changed. The -watch flag enables file watching, which automatically detects changes in the source code and triggers a redeployment. The other commands either do not enable AWS SAM Accelerate mode, or do not redeploy the Lambda functions automatically. Reference: AWS SAM Accelerate"
  },
  {
    "question": "A development team wants to build a continuous integration/continuous delivery (CI/C",
    "options": [
      "pipeline. The team is using AWS CodePipeline to automate the code build and deployment. The team wants to store the program code to prepare for the CI/CD pipeline. Which AWS service should the team use to store the program code?",
      "AWS CodeDeploy",
      "AWS CodeArtifact",
      "AWS CodeCommit Amazon CodeGuru"
    ],
    "answer": "AWS CodeArtifact",
    "explanation": "The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) AWS CodeCommit is a service that provides fully managed source control for hosting secure and scalable private Git repositories. The development team can use CodeCommit to store the program code and prepare for the CI/CD pipeline. CodeCommit integrates with other AWS services such as CodePipeline, CodeBuild, and CodeDeploy to automate the code build and deployment process."
  },
  {
    "question": "A developer is creating an application that includes an Amazon API Gateway REST API in the us-east-2 Region. The developer wants to use Amazon CloudFront and a custom domain name for the API. The developer has acquired an SSL/TLS certificate for the domain from a third-party provider. How should the developer configure the custom domain for the application?",
    "options": [
      "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the AP",
      "Create a DNS A record for the custom domain.",
      "Import the SSL/TLS certificate into CloudFron",
      "Create a DNS CNAME record for the custom domain.",
      "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the AP. Create a DNS CNAME record for the custom domain.. Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Regio. Create a DNS CNAME record for the custom domain."
    ],
    "answer": "Create a DNS CNAME record for the custom domain.",
    "explanation": "Amazon API Gateway is a service that enables developers to create, publish, maintain, monitor, and secure APIs at any scale. Amazon CloudFront is a content delivery network (CDN) service that can improve the performance and security of web applications. The developer can use CloudFront and a custom domain name for the API Gateway REST API. To do so, the developer needs to import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. This is because CloudFront requires certificates from ACM to be in this Region. The developer also needs to create a DNS CNAME record for the custom domain that points to the CloudFront distribution."
  },
  {
    "question": "A developer maintains an Amazon API Gateway REST API. Customers use the API through a frontend UI and Amazon Cognito authentication. The developer has a new version of the API that contains new endpoints and backward- incompatible interface changes. The developer needs to provide beta access to other developers on the team without affecting customers. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "Define a development stage on the API Gateway AP",
      "Instruct the other developers to point the endpoints to the development stage.",
      "Define a new API Gateway API that points to the new API application cod",
      "Instruct the other developers to point the endpoints to the new API.",
      "Implement a query parameter in the API application code that determines which code version to call.. Specify new API Gateway endpoints for the API endpoints that the developer wants to add."
    ],
    "answer": "Define a development stage on the API Gateway AP",
    "explanation": "Amazon API Gateway is a service that enables developers to create, publish, maintain, monitor, and secure APIs at any scale. The developer can define a development stage on the API Gateway API and instruct the other developers to point the endpoints to the development stage. This way, the developer can The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) provide beta access to the new version of the API without affecting customers who use the production stage. This solution will meet the requirements with the least operational overhead."
  },
  {
    "question": "A developer maintains a critical business application that uses Amazon DynamoDB as the primary data store The DynamoDB table contains millions of documents and receives 30- 60 requests each minute The developer needs to perform processing in near-real time on the documents when they are added or updated in the DynamoDB table How can the developer implement this feature with the LEAST amount of change to the existing application code?",
    "options": [
      "Set up a cron job on an Amazon EC2 instance Run a script every hour to query the table for changes and process the documents",
      "Enable a DynamoDB stream on the table Invoke an AWS Lambda function to process the documents.",
      "Update the application to send a PutEvents request to Amazon EventBridg",
      "Create an EventBridge rule to invoke an AWS Lambda function to process the documents.",
      "Update the application to synchronously process the documents directly after the DynamoDB write"
    ],
    "answer": "Enable a DynamoDB stream on the table Invoke an AWS Lambda function to process the documents.",
    "explanation": "https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and- design-patterns/"
  },
  {
    "question": "A company is building a new application that runs on AWS and uses Amazon API Gateway to expose APIs Teams of developers are working on separate so that teams that components of the application in parallel The company wants to publish an API without an integrated backend depend on the application backend can continue the development work before the API backend development is complete. Which solution will meet these requirements?",
    "options": [
      "Create API Gateway resources and set the integration type value to MOCK Configure the method integration request and integration response to associate a response with an HTTP status code Create an API Gateway stage and deploy the API.",
      "Create an AWS Lambda function that returns mocked responses and various HTTP status code",
      "Create API Gateway resources and set the integration type value to AWS_PROXY Deploy the API.",
      "Create an EC2 application that returns mocked HTTP responses Create API Gateway resources and set the integration type value to AWS Create an API Gateway stage and deploy the API.",
      "Create API Gateway resources and set the integration type value set to HTTP_PROX. Add mapping templates and deploy the AP. Create an AWS Lambda layer that returns various HTTP status codes Associate the Lambda layer with the API deployment"
    ],
    "answer": "Create API Gateway resources and set the integration type value to MOCK Configure the method integration request and integration response to associate a response with an HTTP status code Create an API Gateway stage and deploy the API.",
    "explanation": "The best solution for publishing an API without an integrated backend is to use the MOCK integration type in API Gateway. This allows the developer to return a static response to the client without sending the request to a backend service. The developer can configure the method integration request and integration response to associate a response with an HTTP status code, such as 200 OK or 404 Not Found. The developer can also create an API Gateway stage and deploy the API to make it available to the teams that depend on the application backend. The other solutions are either not feasible or not efficient. Creating an AWS Lambda function, an EC2 application, or an AWS Lambda layer would require additional resources and code to generate the mocked responses and HTTP status codes. These solutions would also incur additional costs and complexity, and would not leverage the built-in functionality of API Gateway. References ? Set up mock integrations for API Gateway REST APIs ? Mock Integrat"
  },
  {
    "question": "A company is offering APIs as a service over the internet to provide unauthenticated read access to statistical information that is updated daily. The company uses Amazon API Gateway and AWS Lambda to develop the APIs. The service has become popular, and the company wants to enhance the responsiveness of the APIs. Which action can help the company achieve this goal?",
    "options": [
      "Enable API caching in API Gateway.",
      "Configure API Gateway to use an interface VPC endpoint.",
      "Enable cross-origin resource sharing (CORS) for the APIs.",
      "Configure usage plans and API keys in API Gateway."
    ],
    "answer": "Enable API caching in API Gateway.",
    "explanation": "Amazon API Gateway is a service that enables developers to create, publish, maintain, monitor, and secure APIs at any scale. The developer can enable API caching in API Gateway to cache responses from the backend integration point for a specified time-to- live (TTL) period. This can improve the responsiveness of the APIs by reducing the number of calls made to the backend service."
  },
  {
    "question": "A developer is designing a serverless application for a game in which users register and log in through a web browser The application makes requests on behalf of users to a set of AWS Lambda functions that run behind an Amazon API Gateway HTTP API The developer needs to implement a solution to register and log in users on the application's sign-in page. The solution must minimize operational overhead and must minimize ongoing management of user identities. Which solution will meet these requirements'?",
    "options": [
      "Create Amazon Cognito user pools for external social identity providers Configure 1AM roles for the identity pools.",
      "Program the sign-in page to create users' 1AM groups with the 1AM roles attached to the groups",
      "Create an Amazon RDS for SQL Server DB instance to store the users and manage the permissions to the backend resources in AWS",
      "Configure the sign-in page to register and store the users and their passwords in an Amazon DynamoDB table with an attached IAM policy."
    ],
    "answer": "Create Amazon Cognito user pools for external social identity providers Configure 1AM roles for the identity pools.",
    "explanation": "https://docs.aws.amazon.com/cognito/latest/developerguide/signing-up-users-in-your-app.html"
  },
  {
    "question": "A company is building an application for stock trading. The application needs sub- millisecond latency for processing trade requests. The company uses Amazon DynamoDB to store all the trading data that is used to process each trading request A development team performs load testing on the application and finds that the data retrieval time is higher than expected. The development team needs a solution that reduces the data retrieval time with the least possible effort. Which solution meets these requirements'? The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As)",
    "options": [
      "Add local secondary indexes (LSis) for the trading data.",
      "Store the trading data m Amazon S3 and use S3 Transfer Acceleration.",
      "Add retries with exponential back off for DynamoDB queries.",
      "Use DynamoDB Accelerator (DAX) to cache the trading data."
    ],
    "answer": "Use DynamoDB Accelerator (DAX) to cache the trading data.",
    "explanation": "This solution will meet the requirements by using DynamoDB Accelerator (DAX), which is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10 times performance improvement - from milliseconds to microseconds - even at millions of requests per second. The developer can use DAX to cache the trading data that is used to process each trading request, which will reduce the data retrieval time with the least possible effort. Option A is not optimal because it will add local secondary indexes (LSIs) for the trading data, which may not improve the performance or reduce the latency of data retrieval, as LSIs are stored on the same partition as the base table and share the same provisioned throughput. Option B is not optimal because it will store the trading data in Amazon S3 and use S3 Transfer Acceleration, which is a feature that enables fast, easy, and secure transfers of files over long distances between S3 buckets and clients, not between DynamoDB and clie"
  },
  {
    "question": "An Amazon Simple Queue Service (Amazon SQS) queue serves as an event source for an AWS Lambda function In the SQS queue, each item corresponds to a video file that the Lambda function must convert to a smaller resolution The Lambda function is timing out on longer video files, but the Lambda function's timeout is already configured to its maximum value What should a developer do to avoid the timeouts without additional code changes'?",
    "options": [
      "Increase the memory configuration of the Lambda function",
      "Increase the visibility timeout on the SQS queue",
      "Increase the instance size of the host that runs the Lambda function.",
      "Use multi-threading for the conversion."
    ],
    "answer": "Increase the memory configuration of the Lambda function",
    "explanation": "Increasing the memory configuration of the Lambda function will also increase the CPU and network throughput available to the function. This can improve performance of the video conversion process and reduce the execution time of the function. This solution does not require any code the changes or additional resources. It is also recommended to follow the best practices for preventing Lambda function timeouts1. References ? Troubleshoot Lambda function invocation timeout errors | AWS re:Post"
  },
  {
    "question": "A company is building a compute-intensive application that will run on a fleet of Amazon EC2 instances. The application uses attached Amazon Elastic Block Store (Amazon EBS) volumes for storing data. The Amazon EBS volumes will be created at time of initial deployment. The application will process sensitive information. All of the data must be encrypted. The solution should not impact the application's performance. Which solution will meet these requirements?",
    "options": [
      "Configure the fleet of EC2 instances to use encrypted EBS volumes to store data.",
      "Configure the application to write all data to an encrypted Amazon S3 bucket.",
      "Configure a custom encryption algorithm for the application that will encrypt and decrypt all data.",
      "Configure an Amazon Machine Image (AMI) that has an encrypted root volume and store the data to ephemeral disks."
    ],
    "answer": "Configure the fleet of EC2 instances to use encrypted EBS volumes to store data.",
    "explanation": "Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with Amazon EC2 instances1. Amazon EBS encryption offers a straight- forward encryption solution for your EBS resources associated with your EC2 instances1. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted: Data at rest inside the volume, all data moving between the volume and the instance, all snapshots created from the volume, and all volumes created from those snapshots1. Therefore, option A is correct."
  },
  {
    "question": "A company has an application that runs as a series of AWS Lambda functions. Each Lambda function receives data from an Amazon Simple Notification Service (Amazon SNS) topic and writes the data to an Amazon Aurora DB instance. To comply with an information security policy, the company must ensure that the Lambda functions all use a single securely encrypted database connection string to access Aurora. Which solution will meet these requirements'?",
    "options": [
      "Use IAM database authentication for Aurora to enable secure database connections for ail the Lambda functions.",
      "Store the credentials and read the credentials from an encrypted Amazon RDS DB instance.",
      "Store the credentials in AWS Systems Manager Parameter Store as a secure string parameter.",
      "Use Lambda environment variables with a shared AWS Key Management Service (AWS KMS) key for encryption."
    ],
    "answer": "Use IAM database authentication for Aurora to enable secure database connections for ail the Lambda functions.",
    "explanation": "This solution will meet the requirements by using IAM database authentication for Aurora, which enables using IAM roles or users to authenticate with Aurora databases instead of using passwords or other secrets. The developer can use IAM database authentication for Aurora to enable secure database connections for all the Lambda functions that access Aurora DB instance. The developer can create an IAM role with permission to connect to Aurora DB instance and attach it to each Lambda function. The developer can also configure Aurora DB instance to use IAM database authentication and enable encryption in transit using SSL certificates. This way, the Lambda functions can use a single securely encrypted database connection string to access Aurora without needing any secrets or passwords. Option B is not optimal because it will store the credentials and read them from an encrypted Amazon RDS DB instance, which may introduce additional costs and complexity for managing and accessing another R"
  },
  {
    "question": "A company has an existing application that has hardcoded database credentials A developer needs to modify the existing application The application is deployed in two AWS Regions with an active-passive failover configuration to meet company’s disaster recovery strategy The developer needs a solution to store the credentials outside the code. The solution must comply With the company's disaster recovery strategy Which solution Will meet these requirements in the MOST secure way?",
    "options": [
      "Store the credentials in AWS Secrets Manager in the primary Regio The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As)",
      "Enable secret replication to the secondary Region Update the application to use the Amazon Resource Name (ARN) based on the Region.",
      "Store credentials in AWS Systems Manager Parameter Store in the primary Regio",
      "Enable parameter replication to the secondary Regio"
    ],
    "answer": "Store the credentials in AWS Secrets Manager in the primary Regio The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As)",
    "explanation": "AWS Secrets Manager is a service that allows you to store and manage secrets, such as database credentials, API keys, and passwords, in a secure and centralized way. It also provides features such as automatic secret rotation, auditing, and monitoring1. By using AWS Secrets Manager, you can avoid hardcoding credentials in your code, which is a bad security practice and makes it difficult to update them. You can also replicate your secrets to another Region, which is useful for disaster recovery purposes2. To access your secrets from your application, you can use the ARN of the secret, which is a unique identifier that includes the Region name. This way, your application can use the appropriate secret based on the Region where it is deployed3."
  },
  {
    "question": "A developer at a company needs to create a small application that makes the same API call once each day at a designated time. The company does not have infrastructure in the AWS Cloud yet, but the company wants to implement this functionality on AWS. Which solution meets these requirements in the MOST operationally efficient manner? Use a Kubernetes cron job that runs on Amazon Elastic Kubernetes Service (Amazon EKS).",
    "options": [
      "B. Use an Amazon Linux crontab scheduled job that runs on Amazon EC2.",
      "Use an AWS Lambda function that is invoked by an Amazon EventBridge scheduled event.",
      "Use an AWS Batch job that is submitted to an AWS Batch job queue."
    ],
    "answer": "Use an AWS Batch job that is submitted to an AWS Batch job queue.",
    "explanation": "The correct answer is C. Use an AWS Lambda function that is invoked by an Amazon EventBridge scheduled event. * C. Use an AWS Lambda function that is invoked by an Amazon EventBridge scheduled event. This is correct. AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, and logging1. Amazon EventBridge is a serverless event bus service that enables you to connect your applications with data from a variety of sources2. EventBridge can create rules that run on a schedule, either at regular intervals or at specific times and dates, and invoke targets such as Lambda functions3. This solution meets the requirements of creating a small application that makes the same API call once each day at a designated time, wi"
  },
  {
    "question": "A developer is deploying an AWS Lambda function The developer wants the ability to return to older versions of the function quickly and seamlessly. How can the developer achieve this goal with the LEAST operational overhead?",
    "options": [
      "Use AWS OpsWorks to perform blue/green deployments.",
      "Use a function alias with different versions.",
      "Maintain deployment packages for older versions in Amazon S3. The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As)",
      "Use AWS CodePipeline for deployments and rollbacks."
    ],
    "answer": "Use a function alias with different versions.",
    "explanation": "A function alias is a pointer to a specific Lambda function version. You can use aliases to create different environments for your function, such as development, testing, and production. You can also use aliases to perform blue/green deployments by shifting traffic between two versions of your function gradually. This way, you can easily roll back to a previous version if something goes wrong, without having to redeploy your code or change your configuration. Reference: AWS Lambda function aliases"
  },
  {
    "question": "A company has an ecommerce application. To track product reviews, the company's development team uses an Amazon DynamoDB table. Every record includes the following • A Review ID a 16-digrt universally unique identifier (UUI",
    "options": [
      "• A Product ID and User ID 16 digit UUlDs that reference other tables • A Product Rating on a scale of 1-5 • An optional comment from the user The table partition key is the Review ID. The most performed query against the table is to find the 10 reviews with the highest rating for a given product. The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) Which index will provide the FASTEST response for this query\"?",
      "A global secondary index (GSl) with Product ID as the partition key and Product Rating as the sort key",
      "A global secondary index (GSl) with Product ID as the partition key and Review ID as the sort key",
      "A local secondary index (LSI) with Product ID as the partition key and Product Rating as the sort key",
      "A local secondary index (LSI) with Review ID as the partition key and Product ID as the sort key"
    ],
    "answer": "• A Product ID and User ID 16 digit UUlDs that reference other tables • A Product Rating on a scale of 1-5 • An optional comment from the user The table partition key is the Review ID. The most performed query against the table is to find the 10 reviews with the highest rating for a given product. The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) Which index will provide the FASTEST response for this query\"?",
    "explanation": "This solution allows the fastest response for the query because it enables the query to use a single partition key value (the Product ID) and a range of sort key values (the Product Rating) to find the matching items. A global secondary index (GSI) is an index that has a partition key and an optional sort key that are different from those on the base table. A GSI can be created at any time and can be queried or scanned independently of the base table. A local secondary index (LSI) is an index that has the same partition key as the base table, but a different sort key. An LSI can only be created when the base table is created and must be queried together with the base table partition key. Using a GSI with Product ID as the partition key and Review ID as the sort key will not allow the query to use a range of sort key values to find the highest ratings. Using an LSI with Product ID as the partition key and Product Rating as the sort key will not work because Product ID is not the partiti"
  },
  {
    "question": "A developer warns to add request validation to a production environment Amazon API Gateway API. The developer needs to test the changes before the API is deployed to the production environment. For the lest the developer will send test requests to the API through a testing tool. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "Export the existing API to an OpenAPI fil",
      "Create a new API Import the OpenAPI file Modify the new API to add request validatio",
      "Perform the tests Modify the existing API to add request validatio",
      "Deploy the existing API to production."
    ],
    "answer": "Deploy the existing API to production.",
    "explanation": "This solution allows the developer to test the changes without affecting the production environment. Cloning an API creates a copy of the API definition that can be modified independently. The developer can then add request validation to the new API and test it using a testing tool. After verifying that the changes work as expected, the developer can apply the same changes to the existing API and deploy it to production. Reference: Clone an API, [Enable Request Validation for an API in API Gateway]"
  },
  {
    "question": "A developer is planning to migrate on-premises company data to Amazon S3. The data must be encrypted, and the encryption Keys must support automate annual rotation. The company must use AWS Key Management Service (AWS KMS) to encrypt the data. When type of keys should the developer use to meet these requirements?",
    "options": [
      "Amazon S3 managed keys",
      "Symmetric customer managed keys with key material that is generated by AWS",
      "Asymmetric customer managed keys with key material that generated by AWS",
      "Symmetric customer managed keys with imported key material"
    ],
    "answer": "Symmetric customer managed keys with key material that is generated by AWS",
    "explanation": "The type of keys that the developer should use to meet the requirements is symmetric customer managed keys with key material that is generated by AWS. This way, the developer can use AWS Key Management Service (AWS KMS) to encrypt the data with a symmetric key that is managed by the developer. The developer can also enable automatic annual rotation for the key, which creates new key material for the key every year. The other options either involve using Amazon S3 managed keys, which do not support automatic annual rotation, or using asymmetric keys or imported key material, which are not supported by S3 encryption. Reference: Using AWS KMS keys to encrypt S3 objects"
  },
  {
    "question": "A company has built an AWS Lambda function to convert large image files into output files that can be used in a third-party viewer application The company recently added a new module to the function to improve the output of the generated files However, the new module has increased the bundle size and has increased the time that is needed to deploy changes to the function code. How can a developer increase the speed of the Lambda function deployment?",
    "options": [
      "Use AWS CodeDeploy to deploy the function code",
      "Use Lambda layers to package and load dependencies.",
      "Increase the memory size of the function.",
      "Use Amazon S3 to host the function dependencies"
    ],
    "answer": "Use Lambda layers to package and load dependencies.",
    "explanation": "Using Lambda layers is a way to reduce the size of the deployment package and speed up the deployment process. Lambda layers are reusable components that can contain libraries, custom runtimes, or other dependencies. By using layers, the developer can separate the core function logic from the dependencies, and avoid uploading them every time the function code changes. Layers can also be shared across multiple functions or accounts, which can improve consistency and maintainability. References ? Working with AWS Lambda layers ? AWS Lambda Layers Best Practices ? Best practices for working with AWS Lambda functions"
  },
  {
    "question": "A developer is designing a serverless application with two AWS Lambda functions to process photos. One Lambda function stores objects in an Amazon S3 bucket and stores the associated metadata in an Amazon DynamoDB table. The other Lambda function fetches the objects from the S3 bucket by using the metadata from the DynamoDB table. Both Lambda functions use the same Python library to perform complex computations and are approaching the quota for the maximum size of zipped deployment packages. The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) What should the developer do to reduce the size of the Lambda deployment packages with the LEAST operational overhead?",
    "options": [
      "Package each Python library in its own .zip file archiv",
      "Deploy each Lambda function with its own copy of the library.",
      "Create a Lambda layer with the required Python librar",
      "Use the Lambda layer in both Lambda functions."
    ],
    "answer": "Deploy each Lambda function with its own copy of the library.",
    "explanation": "AWS Lambda is a service that lets developers run code without provisioning or managing servers. Lambda layers are a distribution mechanism for libraries, custom runtimes, and other dependencies. The developer can create a Lambda layer with the required Python library and use the layer in both Lambda functions. This will reduce the size of the Lambda deployment packages and avoid reaching the quota for the maximum size of zipped deployment packages. The developer can also benefit from using layers to manage dependencies separately from function code."
  },
  {
    "question": "A company developed an API application on AWS by using Amazon CloudFront, Amazon API Gateway, and AWS Lambda. The API has a minimum of four requests every second. A developer notices that many API users run the same query by using the POST method. The developer wants to cache the POST request to optimize the API resources. Which solution will meet these requirements?",
    "options": [
      "Configure the CloudFront cach",
      "Update the application to return cached content based upon the default request headers.",
      "Override the cache method in the selected stage of API Gatewa",
      "Select the POST method.",
      "Save the latest request response in Lambda /tmp director. Update the Lambda function to check the /tmp directory.. Save the latest request in AWS Systems Manager Parameter Stor. Modify the Lambda function to take the latest request response from Parameter Store. The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As)"
    ],
    "answer": "Update the application to return cached content based upon the default request headers.",
    "explanation": "Amazon API Gateway provides tools for creating and documenting web APIs that route HTTP requests to Lambda functions2. You can secure access to your API with authentication and authorization controls. Your APIs can serve traffic over the internet or can be accessible only within your VPC2. You can override the cache method in the selected stage of API Gateway2. Therefore, option B is correct."
  },
  {
    "question": "A developer wants to deploy a new version of an AWS Elastic Beanstalk application. During deployment, the application must maintain full capacity and avoid service interruption. Additionally, the developer must minimize the cost of additional resources that support the deployment. Which deployment method should the developer use to meet these requirements?",
    "options": [
      "All at once",
      "Rolling with additional batch",
      "Blue/green",
      "Immutable"
    ],
    "answer": "Immutable",
    "explanation": "The immutable deployment method is the best option for this scenario, because it meets the requirements of maintaining full capacity, avoiding service interruption, and minimizing the cost of additional resources. The immutable deployment method creates a new set of instances in a separate Auto Scaling group and deploys the new version of the application to them. Then, it swaps the new instances with the old ones and terminates the old instances. This way, the application maintains full capacity during the deployment and avoids any downtime. The cost of additional resources is also minimized, because the new instances are only created for a short time and then replaced by the old ones. The other deployment methods do not meet all the requirements: ? The all at once method deploys the new version to all instances simultaneously, which causes a short period of downtime and reduced capacity. ? The rolling with additional batch method deploys the new version in batches, but for the first b"
  },
  {
    "question": "The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) A developer is creating a serverless application that uses an AWS Lambda function The developer will use AWS CloudFormation to deploy the application The application will write logs to Amazon CloudWatch Logs The developer has created a log group in a CloudFormation template for the application to use The developer needs to modify the CloudFormation template to make the name of the log group available to the application at runtime Which solution will meet this requirement?",
    "options": [
      "Use the AWS:lnclude transform in CloudFormation to provide the log group's name to the application",
      "Pass the log group's name to the application in the user data section of the CloudFormation template.",
      "Use the CloudFormation template's Mappings section to specify the log group's name for the application.",
      "Pass the log group's Amazon Resource Name (ARN) as an environment variable to the Lambda function"
    ],
    "answer": "Pass the log group's Amazon Resource Name (ARN) as an environment variable to the Lambda function",
    "explanation": "FunctionName: MyLambdaFunction Code: S3Bucket: your-lambda-code-bucket S3Key: lambda-code.zip Runtime: nodejs14.x # Specify the desired runtime for your Lambda function Environment: Variables: LOG_GROUP_NAME: !Ref MyLogGroup https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs- loggroup.html"
  },
  {
    "question": "A developer is building an application that gives users the ability to view bank account from multiple sources in a single dashboard. The developer has automated the process to retrieve API credentials for these sources. The process invokes an AWS Lambda function that is associated with an AWS CloudFormation cotton resource. The developer wants a solution that will store the API credentials with minimal operational overhead. When solution will meet these requirements?",
    "options": [
      "Add an AWS Secrets Manager GenerateSecretString resource to the CloudFormation templat",
      "Set the value to reference new credentials to the Cloudformation resource.",
      "Use the AWS SDK ssm PutParameter operation in the Lambda function from the existing, custom resource to store the credentials as a paramete",
      "Set the parameter value to reference the new credential"
    ],
    "answer": "Set the value to reference new credentials to the Cloudformation resource.",
    "explanation": "The solution that will meet the requirements is to use the AWS SDK ssm PutParameter operation in the Lambda function from the existing custom resource to store the credentials as a parameter. Set the parameter value to reference the new credentials. Set the parameter type to SecureString. This way, the developer can store the API credentials with minimal operational overhead, as AWS Systems Manager Parameter Store provides secure and scalable storage for configuration data. The SecureString parameter type encrypts the parameter value with AWS Key Management Service (AWS KMS). The other options either involve adding additional resources to the CloudFormation template, which increases complexity and cost, or do not encrypt the parameter value, which reduces security. Reference: Creating Systems Manager parameters"
  },
  {
    "question": "The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) A company hosts its application on AWS. The application runs on an Amazon Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. The cluster runs behind an Application Load Balancer The application stores data in an Amazon Aurora database A developer encrypts and manages database credentials inside the application The company wants to use a more secure credential storage method and implement periodic credential rotation. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "Migrate the secret credentials to Amazon RDS parameter group",
      "Encrypt the parameter by using an AWS Key Management Service (AWS KMS) key Turn on secret rotatio",
      "Use 1AM policies and roles to grant AWS KMS permissions to access Amazon RDS.",
      "Migrate the credentials to AWS Systems Manager Parameter Stor"
    ],
    "answer": "Migrate the credentials to AWS Systems Manager Parameter Stor",
    "explanation": "AWS Secrets Manager is a service that helps you store, distribute, and rotate secrets securely. You can use Secrets Manager to migrate your credentials from your application code to a secure and encrypted storage. You can also enable automatic rotation of your secrets by using AWS Lambda functions or custom logic. You can use IAM policies and roles to grant your Amazon ECS Fargate tasks permissions to access your secrets from Secrets Manager. This solution minimizes the operational overhead of managing your credentials and enhances the security of your application. References ? AWS Secrets Manager: Store, Distribute, and Rotate Credentials Securely | AWS News Blog ? Why You Should Audit and Rotate Your AWS Credentials Periodically - Cloud Academy ? Top 5 AWS root account best practices - TheServerSide"
  },
  {
    "question": "A company has an Amazon S3 bucket that contains sensitive data. The data must be encrypted in transit and at rest. The company The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) encrypts the data in the S3 bucket by using an AWS Key Management Service (AWS KMS) key. A developer needs to grant several other AWS accounts the permission to use the S3 GetObject operation to retrieve the data from the S3 bucket. How can the developer enforce that all requests to retrieve the data provide encryption in transit?",
    "options": [
      "Define a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false”.",
      "Define a resource-based policy on the S3 bucket to allow access when a request meets the condition “aws:SecureTransport”: “false”.",
      "Define a role-based policy on the other accounts' roles to deny access when a request meets the condition of “aws:SecureTransport”: “false”.",
      "Define a resource-based policy on the KMS key to deny access when a request meets the condition of “aws:SecureTransport”: “false”."
    ],
    "answer": "Define a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false”.",
    "explanation": "Amazon S3 supports resource-based policies, which are JSON documents that specify the permissions for accessing S3 resources. A resource-based policy can be used to enforce encryption in transit by denying access to requests that do not use HTTPS. The condition key aws:SecureTransport can be used to check if the request was sent using SSL. If the value of this key is false, the request is denied; otherwise, the request is allowed. Reference: How do I use an S3 bucket policy to require requests to use Secure Socket Layer (SSL)?"
  },
  {
    "question": "A company has developed a new serverless application using AWS Lambda functions that will be deployed using the AWS Serverless Application Model (AWS SAM) CLI. Which step should the developer complete prior to deploying the application?",
    "options": [
      "Compress the application to a zip file and upload it into AWS Lambda.",
      "Test the new AWS Lambda function by first tracing it m AWS X-Ray.",
      "Bundle the serverless application using a SAM package.",
      "Create the application environment using the eb create my-env command."
    ],
    "answer": "Bundle the serverless application using a SAM package.",
    "explanation": "This step should be completed prior to deploying the application because it prepares the application artifacts for deployment. The AWS Serverless Application Model (AWS SAM) is a framework that simplifies building and deploying serverless applications on AWS. The AWS SAM CLI is a command-line tool that helps you create, test, and deploy serverless applications using AWS SAM templates. The sam package command bundles the application artifacts, such as Lambda function code and API definitions, and uploads them to an Amazon S3 bucket. The command also returns a CloudFormation template that is ready to be deployed with the sam deploy command. Compressing the application to a zip file and uploading it to AWS Lambda will not work because it does not use AWS SAM templates or CloudFormation. Testing the new Lambda function by first tracing it in AWS X- Ray will not prepare the application for deployment, but only monitor its The Leader of IT Certification 100% Valid and Newest Version DVA-C02 "
  },
  {
    "question": "An application uses Lambda functions to extract metadata from files uploaded to an S3 bucket; the metadata is stored in Amazon DynamoD",
    "options": [
      "The application starts behaving unexpectedly, and the developer wants to examine the logs of the Lambda function code for errors. Based on this system configuration, where would the developer find the logs?",
      "Amazon S3",
      "AWS CloudTrail",
      "Amazon CloudWatch",
      "Amazon DynamoDB"
    ],
    "answer": "AWS CloudTrail",
    "explanation": "Amazon CloudWatch is the service that collects and stores logs from AWS Lambda functions. The developer can use CloudWatch Logs Insights to query and analyze the logs for errors and metrics. Option A is not correct because Amazon S3 is a storage service that does not store Lambda function logs. Option B is not correct because AWS CloudTrail is a service that records API calls and events for AWS services, not Lambda function logs. Option D is not correct because Amazon DynamoDB is a database service that does not store Lambda function logs."
  },
  {
    "question": "A developer is building a new application on AWS. The application uses an AWS Lambda function that retrieves information from an Amazon DynamoDB table. The developer hard coded the DynamoDB table name into the Lambda function code. The table name might change over time. The developer does not want to modify the Lambda code if the table name changes. Which solution will meet these requirements MOST efficiently?",
    "options": [
      "Create a Lambda environment variable to store the table nam",
      "Use the standard method for the programming language to retrieve the variable.",
      "Store the table name in a fil",
      "Store the file in the /tmp folde"
    ],
    "answer": "Create a Lambda environment variable to store the table nam",
    "explanation": "The solution that will meet the requirements most efficiently is to create a Lambda environment variable to store the table name. Use the standard method for the programming language to retrieve the variable. This way, the developer can avoid hard- coding the table name in the Lambda function code and easily change the table name by updating the environment variable. The other options either involve storing the table name in a file, which is less efficient and secure than using an environment variable, or creating a global variable, which is not recommended as it can cause concurrency issues. Reference: Using AWS Lambda environment variables"
  },
  {
    "question": "A developer is using an AWS Lambda function to generate avatars for profile pictures that are uploaded to an Amazon S3 bucket. The Lambda function is automatically invoked for profile pictures that are saved under the /original/ S3 prefix. The developer notices that some pictures cause the Lambda function to time out. The developer wants to implement a fallback mechanism by using another Lambda function that resizes the profile picture. Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      "Set the image resize Lambda function as a destination of the avatar generator Lambda function for the events that fail processing.",
      "Create an Amazon Simple Queue Service (Amazon SQS) queu",
      "Set the SQS queue as a destination with an on failure condition for the avatar generator Lambda function The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As)",
      "Configure the image resize Lambda function to poll from the SQS queue."
    ],
    "answer": "Set the image resize Lambda function as a destination of the avatar generator Lambda function for the events that fail processing.",
    "explanation": "The solution that will meet the requirements with the least development effort is to set the image resize Lambda function as a destination of the avatar generator Lambda function for the events that fail processing. This way, the fallback mechanism is automatically triggered by the Lambda service without requiring any additional components or configuration. The other options involve creating and managing additional resources such as queues, topics, state machines, or rules, which would increase the complexity and cost of the solution. Reference: Using AWS Lambda destinations"
  },
  {
    "question": "An AWS Lambda function requires read access to an Amazon S3 bucket and requires read/write access to an Amazon DynamoDB table The correct 1AM policy already exists What is the MOST secure way to grant the Lambda function access to the S3 bucket and the DynamoDB table?",
    "options": [
      "Attach the existing 1AM policy to the Lambda function.",
      "Create an 1AM role for the Lambda function Attach the existing 1AM policy to the role Attach the role to the Lambda function",
      "Create an 1AM user with programmatic access Attach the existing 1AM policy to the use",
      "Add the user access key ID and secret access key as environment variables in the Lambda function.",
      "Add the AWS account root user access key ID and secret access key as encrypted environment variables in the Lambda function"
    ],
    "answer": "Create an 1AM role for the Lambda function Attach the existing 1AM policy to the role Attach the role to the Lambda function",
    "explanation": "The most secure way to grant the Lambda function access to the S3 bucket and the DynamoDB table is to create an IAM role for the Lambda function and attach the existing IAM policy to the role. This way, you can use the principle of least privilege and avoid exposing any credentials in your function code or environment variables. You can also leverage the temporary security credentials that AWS provides to the Lambda function when it assumes the role. This solution follows the best practices for working with AWS Lambda functions1 and designing and architecting with DynamoDB2. References ? Best practices for working with AWS Lambda functions ? Best practices for designing and architecting with DynamoDB"
  },
  {
    "question": "A company is planning to securely manage one-time fixed license keys in AWS. The company's development team needs to access the license keys in automaton scripts that run in Amazon EC2 instances and in AWS CloudFormation stacks. Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "Amazon S3 with encrypted files prefixed with “config”",
      "AWS Secrets Manager secrets with a tag that is named SecretString",
      "AWS Systems Manager Parameter Store SecureString parameters",
      "CloudFormation NoEcho parameters"
    ],
    "answer": "AWS Systems Manager Parameter Store SecureString parameters",
    "explanation": "AWS Systems Manager Parameter Store is a service that provides secure, hierarchical storage for configuration data and secrets. Parameter Store supports SecureString parameters, which are encrypted using AWS Key Management Service (AWS KMS) keys. SecureString parameters can be used to store license keys in AWS and retrieve them securely from automation scripts that run in EC2 instances or CloudFormation stacks. Parameter Store is a cost-effective solution because it does not charge for storing parameters or API calls. Reference: Working with Systems Manager parameters"
  },
  {
    "question": "An application runs on multiple EC2 instances behind an EL",
    "options": [
      "Where is the session data best written so that it can be served reliably across multiple requests?",
      "Write data to Amazon ElastiCache",
      "Write data to Amazon Elastic Block Store",
      "Write data to Amazon EC2 instance Store",
      "Wide data to the root filesystem"
    ],
    "answer": "Where is the session data best written so that it can be served reliably across multiple requests?",
    "explanation": "The solution that will meet the requirements is to write data to Amazon ElastiCache. This way, the application can write session data to a fast, scalable, and reliable in-memory data store that can be served reliably across multiple requests. The other options either involve writing data to persistent storage, which is slower and more expensive than in-memory storage, or writing data to the root filesystem, which is not shared among multiple EC2 instances. Reference: Using ElastiCache for session management"
  },
  {
    "question": "An developer is building a serverless application by using the AWS Serverless Application Model (AWS SAM). The developer is currently testing the application in a development environment. When the application is nearly finsihed, the developer will need to set up additional testing and staging environments for a quality assurance team. The developer wants to use a feature of the AWS SAM to set up deployments to multiple environments. Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      "Add a configuration file in TOML format to group configuration entries to every environmen",
      "Add a table for each testing and staging environmen",
      "Deploy updates to the environments by using the sam deploy command and the --config-env flag that corresponds to the each environment.",
      "Create additional AWS SAM templates for each testing and staging environmen"
    ],
    "answer": "Add a configuration file in TOML format to group configuration entries to every environmen",
    "explanation": "The correct answer is A. Add a configuration file in TOML format to group configuration entries to every environment. Add a table for each testing and staging environment. Deploy updates to the environments by using the sam deploy command and the --config-env flag that corresponds to the each environment. * A. Add a configuration file in TOML format to group configuration entries to every environment. Add a table for each testing and staging environment. Deploy updates to the environments by using the sam deploy command and the --config-env flag that corresponds to the each environment. This is correct. This solution will meet the requirements with the least development effort, because it uses a feature of the AWS SAM CLI that supports a project-level configuration file that can be used to configure AWS SAM CLI command parameter values1. The configuration file can have multiple environments, each with its own set of parameter values, such as stack name, region, capabilities, and more2."
  },
  {
    "question": "A company runs an application on AWS The application stores data in an Amazon DynamoDB table Some queries are taking a long time to run These slow queries involve an attribute that is not the table's partition key or sort key The amount of data that the application stores in the DynamoDB table is expected to increase significantly. A developer must increase the performance of the queries. Which solution will meet these requirements'?",
    "options": [
      "Increase the page size for each request by setting the Limit parameter to be higher than the default value Configure the application to retry any request that exceeds the provisioned throughput.",
      "Create a global secondary index (GSI). Set query attribute to be the partition key of the index",
      "Perform a parallel scan operation by issuing individual scan requests in the parameters specify the segment for the scan requests and the total number of segments for the parallel scan.",
      "Turn on read capacity auto scaling for the DynamoDB tabl",
      "Increase the maximum read capacity units (RCUs)."
    ],
    "answer": "Create a global secondary index (GSI). Set query attribute to be the partition key of the index",
    "explanation": "Creating a global secondary index (GSI) is the best solution to improve the performance of the queries that involve an attribute that is not the table’s partition key or sort key. A GSI allows you to define an alternate key for your table and query the data using that key. This way, you can avoid scanning the entire table and reduce the latency and cost of your queries. You should also follow the best practices for designing and using GSIs in DynamoDB12. References ? Working with Global Secondary Indexes - Amazon DynamoDB ? DynamoDB Performance & Latency - Everything You Need To Know"
  },
  {
    "question": "A company built a new application in the AWS Cloud. The company automated the bootstrapping of new resources with an Auto Scaling group by using AWS Cloudf-ormation templates. The bootstrap scripts contain sensitive data. The company needs a solution that is integrated with CloudFormation to manage the sensitive data in the bootstrap scripts. Which solution will meet these requirements in the MOST secure way?",
    "options": [
      "Put the sensitive data into a CloudFormation paramete",
      "Encrypt the CloudFormation templates by using an AWS Key Management Service (AWS KMS) key.",
      "Put the sensitive data into an Amazon S3 bucket Update the CloudFormation templates to download the object from Amazon S3 during bootslrap.",
      "Put the sensitive data into AWS Systems Manager Parameter Store as a secure string paramete",
      "Update the CloudFormation templates to use dynamic references to specify template values. encryption after file system creatio. Put the sensitive data into Amazon Elastic File System (Amazon EPS) Enforce EFS. Update the CloudFormation templates to retrieve data from Amazon EFS."
    ],
    "answer": "Put the sensitive data into an Amazon S3 bucket Update the CloudFormation templates to download the object from Amazon S3 during bootslrap.",
    "explanation": "This solution meets the requirements in the most secure way because it uses a service that is integrated with CloudFormation to manage sensitive data in encrypted form. AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store sensitive data as secure string parameters, which are encrypted using an AWS Key Management Service (AWS KMS) key of your choice. You can also use dynamic references in your CloudFormation templates to specify template values that are stored in Parameter Store or Secrets Manager without having to include them in your templates. Dynamic references are resolved only during stack creation or update operations, which reduces exposure risks for sensitive data. The Leader of IT Certification 100% Valid and Newest Version DVA-C02 Questions & Answers shared by Certleader (127 Q&As) Putting sensitive data into a CloudFormation parameter will not encrypt them or protect them from unauth"
  },
  {
    "question": "A developer is storing sensitive data generated by an application in Amazon S3. The developer wants to encrypt the data at rest. A company policy requires an audit trail of when the AWS Key Management Service (AWS KMS) key was used and by whom. Which encryption option will meet these requirements?",
    "options": [
      "Server-side encryption with Amazon S3 managed keys (SSE-S3) Server-side encryption with AWS KMS managed keys (SSE-KMS}",
      "C. Server-side encryption with customer-provided keys (SSE-C)",
      "Server-side encryption with self-managed keys"
    ],
    "answer": "C. Server-side encryption with customer-provided keys (SSE-C)",
    "explanation": "This solution meets the requirements because it encrypts data at rest using AWS KMS keys and provides an audit trail of when and by whom they were used. Server- side encryption with AWS KMS managed keys (SSE-KMS) is a feature of Amazon S3 that encrypts data using keys that are managed by AWS KMS. When SSE-KMS is enabled for an S3 bucket or object, S3 requests AWS KMS to generate data keys and encrypts data using these keys. AWS KMS logs every use of its keys in AWS CloudTrail, which records all API calls to AWS KMS as events. These events include information such as who made the request, when it was made, and which key was used. The company policy can use CloudTrail logs to audit critical events related to their data encryption and access. Server- side encryption with Amazon S3 managed keys (SSE-S3) also encrypts data at rest using keys that are managed by S3, but does not provide an audit trail of key usage. Server-side encryption with customer-provided keys (SSE-C) and server-side en"
  },
  {
    "question": "A company has an application that runs across multiple AWS Regions. The application is experiencing performance issues at irregular intervals. A developer must to implement distributed tracing for the application to troubleshoot the root cause of the performance issues. use AWS X-Ray What should the developer do to meet this requirement?",
    "options": [
      "Use the X-Ray console to add annotations for AWS services and user-defined services",
      "Use Region annotation that X-Ray adds automatically for AWS services Add Region annotation for user-defined services",
      "Use the X-Ray daemon to add annotations for AWS services and user-defined services",
      "Use Region annotation that X-Ray adds automatically for user-defined services Configure X-Ray to add Region annotation for AWS services"
    ],
    "answer": "Use Region annotation that X-Ray adds automatically for AWS services Add Region annotation for user-defined services",
    "explanation": "AWS X-Ray automatically adds Region annotation for AWS services that are integrated with X-Ray. This annotation indicates the AWS Region where the service is running. The developer can use this annotation to filter and group traces by Region and identify any performance issues related to cross-Region calls. The developer can also add Region annotation for user-defined services by using the X-Ray SDK. This option enables the developer to implement distributed tracing for the application that runs across multiple AWS Regions. References ? AWS X-Ray Annotations ? AWS X-Ray Concepts"
  },
  {
    "question": "A developer has been asked to create an AWS Lambda function that is invoked any time updates are made to items in an Amazon DynamoDB table. The function has been created and appropriate permissions have been added to the Lambda execution role Amazon DynamoDB streams have been enabled for the table, but invoked. the function 15 still not being Which option would enable DynamoDB table updates to invoke the Lambda function?",
    "options": [
      "Change the StreamViewType parameter value to NEW_AND_OLOJMAGES for the DynamoDB table.",
      "Configure event source mapping for the Lambda function.",
      "Map an Amazon Simple Notification Service (Amazon SNS) topic to the DynamoDB streams.",
      "Increase the maximum runtime (timeout) setting of the Lambda function."
    ],
    "answer": "Configure event source mapping for the Lambda function.",
    "explanation": "This solution allows the Lambda function to be invoked by the DynamoDB stream whenever updates are made to items in the DynamoDB table. Event source mapping is a feature of Lambda that enables a function to be triggered by an event source, such as a DynamoDB stream, an Amazon Kinesis stream, or an Amazon Simple Queue Service (SQS) queue. The developer can configure event source mapping for the Lambda function using the AWS Management Console, the AWS CLI, or the AWS SDKs. Changing the StreamViewType parameter value to NEW_AND_OLD_IMAGES for the DynamoDB table will not affect the invocation of the Lambda function, but only change the information that is written to the stream record. Mapping an Amazon Simple Notification Service (Amazon SNS) topic to the DynamoDB stream will not invoke the Lambda function directly, but require an additional subscription from the Lambda function to the SNS topic. Increasing the maximum runtime (timeout) setting of the Lambda function will not affect the i"
  },
  {
    "question": "A developer is creating an AWS Lambda function that needs credentials to connect to an Amazon RDS for MySQL database. An Amazon S3 bucket currently stores the credentials. The developer needs to improve the existing solution by implementing credential rotation and secure storage. The developer also needs to provide integration with the Lambda function. Which solution should the developer use to store and retrieve the credentials with the LEAST management overhead?",
    "options": [
      "Store the credentials in AWS Systems Manager Parameter Stor",
      "Select the database that the parameter will acces",
      "Use the default AWS Key Management Service (AWS KMS) key to encrypt the paramete",
      "Enable automatic rotation for the paramete"
    ],
    "answer": "Use the default AWS Key Management Service (AWS KMS) key to encrypt the paramete",
    "explanation": "AWS Secrets Manager is a service that helps you protect secrets needed to access your applications, services, and IT resources. Secrets Manager enables you to store, retrieve, and rotate secrets such as database credentials, API keys, and passwords. Secrets Manager supports a secret type for RDS databases, which allows you to select an existing RDS database instance and generate credentials for it. Secrets Manager encrypts the secret using AWS Key Management Service (AWS KMS) keys and enables automatic rotation of the secret at a specified interval. A Lambda function can use the AWS SDK or CLI to retrieve the secret from Secrets Manager and use it to connect to the database. Reference: Rotating your AWS Secrets Manager secrets"
  },
  {
    "question": "A developer is creating a mobile application that will not require users to log in. What is the MOST efficient method to grant users access to AWS resources'?",
    "options": [
      "Use an identity provider to securely authenticate with the application.",
      "Create an AWS Lambda function to create an 1AM user when a user accesses the application.",
      "Create credentials using AWS KMS and apply these credentials to users when using the application.",
      "Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources."
    ],
    "answer": "Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources.",
    "explanation": "This solution is the most efficient method to grant users access to AWS resources without requiring them to log in. Amazon Cognito is a service that provides user sign-up, sign-in, and access control for web and mobile applications. Amazon Cognito identity pools support both authenticated and unauthenticated users. Unauthenticated users receive access to your AWS resources even if they aren’t logged in with any of your identity providers (IdPs). You can use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources, such as Amazon S3 buckets or DynamoDB tables. This degree of access is useful to display content to users before they log in or to allow them to perform certain actions without signing up. Using an identity provider to securely authenticate with the application will require users to log in, which does not meet the requirement. Creating an AWS Lambda function to create an IAM user when a user accesses the application will incur un"
  },
  {
    "question": "A developer is creating an AWS CloudFormation template to deploy Amazon EC2 instances across multiple AWS accounts. The developer must choose the EC2 instances from a list of approved instance types. How can the developer incorporate the list of approved instance types in the CloudFormation template?",
    "options": [
      "Create a separate CloudFormation template for each EC2 instance type in the list.",
      "In the Resources section of the CloudFormation template, create resources for each EC2 instance type in the list.",
      "In the CloudFormation template, create a separate parameter for each EC2 instance type in the list.",
      "In the CloudFormation template, create a parameter with the list of EC2 instance types as AllowedValues."
    ],
    "answer": "In the CloudFormation template, create a parameter with the list of EC2 instance types as AllowedValues.",
    "explanation": "In the CloudFormation template, the developer should create a parameter with the list of approved EC2 instance types as AllowedValues. This way, users can select the instance type they want to use when launching the CloudFormation stack, but only from the approved list."
  },
  {
    "question": "A developer is working on an ecommerce platform that communicates with several third- party payment processing APIs The third-party payment services do not provide a test environment. The developer needs to validate the ecommerce platform's integration with the third-party payment processing APIs. The developer must test the API integration code without invoking the third-party payment processing APIs. Which solution will meet these requirements'?",
    "options": [
      "Set up an Amazon API Gateway REST API with a gateway response configured for status code 200 Add response templates that contain sample responses captured from the real third-party API.",
      "Set up an AWS AppSync GraphQL API with a data source configured for each third- party API Specify an integration type of Mock Configure integration responses by using sample responses captured from the real third-party API.",
      "Create an AWS Lambda function for each third-party AP",
      "Embed responses captured from the real third-party AP",
      "Configure Amazon Route 53 Resolver with an inbound endpoint for each Lambda function's Amazon Resource Name (ARN).. Set up an Amazon API Gateway REST API for each third-party API Specify an integration request type of Mock Configure integration responses by using sample responses captured from the real third-party API"
    ],
    "answer": "Embed responses captured from the real third-party AP",
    "explanation": "Amazon API Gateway can mock responses for testing purposes without requiring any integration backend. This allows the developer to test the API integration code without invoking the third-party payment processing APIs. The developer can configure integration responses by using sample responses captured from the real third- party API."
  },
  {
    "question": "A company is migrating an on-premises database to Amazon RDS for MySQL. The company has read-heavy workloads. The company wants to refactor the code to achieve optimum read performance for queries. Which solution will meet this requirement with LEAST current and future effort? Use a multi-AZ Amazon RDS deploymen",
    "options": [
      "B. Increase the number of connections that the code makes to the database or increase the connection pool size if a connection pool is in use.",
      "Use a multi-AZ Amazon RDS deploymen",
      "Modify the code so that queries access the secondary RDS instance.",
      "Deploy Amazon RDS with one or more read replica. Modify the application code so that queries use the URL for the read replicas.. Use open source replication software to create a copy of the MySQL database on an Amazon EC2 instanc. Modify the application code so that queries use the IP address of the EC2 instance."
    ],
    "answer": "Modify the code so that queries access the secondary RDS instance.",
    "explanation": "Amazon RDS for MySQL supports read replicas, which are copies of the primary database instance that can handle read-only queries. Read replicas can improve the read performance of the database by offloading the read workload from the primary instance and distributing it across multiple replicas. To use read replicas, the application code needs to be modified to direct read queries to the URL of the read replicas, while write queries still go to the URL of the primary instance. This solution requires less current and future effort than using a multi-AZ deployment, which does not provide read scaling benefits, or using open source replication software, which requires additional configuration and maintenance. Reference: Working with read replicas"
  },
  {
    "question": "A developer is building a serverless application by using AWS Serverless Application Model (AWS SAM) on multiple AWS Lambda functions. When the application is deployed, the developer wants to shift 10% of the traffic to the new deployment of the application for the first 10 minutes after deployment. If there are no issues, all traffic must switch over to the new version. Which change to the AWS SAM template will meet these requirements?",
    "options": [
      "Set the Deployment Preference Type to Canary10Percent10Minute AutoPublishAlias property to the Lambda alias.",
      "Set the",
      "Set the Deployment Preference Type to LinearlOPercentEvery10Minute",
      "Set AutoPubIishAIias property to the Lambda alias.",
      "Set the Deployment Preference Type to CanaryIOPercentIOMinute. Set the PreTraffic and PostTraffic properties to the Lambda alias.. Set the Deployment Preference Type to LinearlOPercentEveryIOMinute. Set PreTraffic and Post Traffic properties to the Lambda alias."
    ],
    "answer": "Set the Deployment Preference Type to Canary10Percent10Minute AutoPublishAlias property to the Lambda alias.",
    "explanation": "The AWS Serverless Application Model (AWS SAM) comes built-in with CodeDeploy to provide gradual AWS Lambda deployments1. The DeploymentPreference property in AWS SAM allows you to specify the type of deployment that you want. The Canary10Percent10Minutes option means that 10 percent of your customer traffic is immediately shifted to your new version. After 10 minutes, all traffic is shifted to the new version1. The AutoPublishAlias property in AWS SAM allows AWS SAM to automatically create an alias that points to the updated version of the Lambda function1. Therefore, option A is correct."
  },
  {
    "question": "A developer is working on a web application that uses Amazon DynamoDB as its data store The application has two DynamoDB tables one table that is named artists and one table that is named songs The artists table has artistName as the partition key. The songs table has songName as the partition key and artistName as the sort key The table usage patterns include the retrieval of multiple songs and artists in a single database operation from the webpage. The developer needs a way to retrieve this information with minimal network traffic and optimal application performance. Which solution will meet these requirements'?",
    "options": [
      "Perform a BatchGetltem operation that returns items from the two table",
      "Use the list of songName artistName keys for the songs table and the list of artistName key for the artists table.",
      "Create a local secondary index (LSI) on the songs table that uses artistName as the partition key Perform a query operation for each artistName on the songs",
      "Perform a BatchGetltem operation on the songs table that uses the songName/artistName key",
      "Perform a BatchGetltem operation on the artists table that uses artistName as the key.. Perform a Scan operation on each table that filters by the list of songName/artistName for the songs table and the list of artistName in the artists table."
    ],
    "answer": "Perform a BatchGetltem operation that returns items from the two table",
    "explanation": "BatchGetItem can return one or multiple items from one or more tables. For reference check the link below https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.h tml"
  },
  {
    "question": "A developer has a legacy application that is hosted on-premises. Other applications hosted on AWS depend on the on-premises application for proper functioning. In case of any application errors, the developer wants to be able to use Amazon CloudWatch to monitor and troubleshoot all applications from one place. How can the developer accomplish this?",
    "options": [
      "Install an AWS SDK on the on-premises server to automatically send logs to CloudWatch.",
      "Download the CloudWatch agent to the on-premises serve",
      "Configure the agent to use IAM user credentials with permissions for CloudWatch.",
      "Upload log files from the on-premises server to Amazon S3 and have CloudWatch read the files.",
      "Upload log files from the on-premises server to an Amazon EC2 instance and have the instance forward the logs to CloudWatch."
    ],
    "answer": "Download the CloudWatch agent to the on-premises serve",
    "explanation": "Amazon CloudWatch is a service that monitors AWS resources and applications. The developer can use CloudWatch to monitor and troubleshoot all applications from one place. To do so, the developer needs to download the CloudWatch agent to the on-premises server and configure the agent to use IAM user credentials with permissions for CloudWatch. The agent will collect logs and metrics from the on-premises server and send them to CloudWatch."
  },
  {
    "question": "A developer deployed an application to an Amazon EC2 instance The application needs to know the public IPv4 address of the instance How can the application find this information? Query the instance metadata from http./M69.254.169.254. latestmeta-data/.",
    "options": [
      "B. Query the instance user data from http '169 254.169 254. latest/user-data/",
      "Query the Amazon Machine Image (AMI) information from http://169.254.169.254/latest/meta-data/ami/.",
      "Check the hosts file of the operating system"
    ],
    "answer": "B. Query the instance user data from http '169 254.169 254. latest/user-data/",
    "explanation": "The instance metadata service provides information about the EC2 instance, including the public IPv4 address, which can be obtained by querying the endpoint http://169.254.169.254/latest/meta-data/public-ipv4. References ? Instance metadata and user data ? Get Public IP Address on current EC2 Instance ? Get the public ip address of your EC2 instance quickly"
  },
  {
    "question": "A team of developed is using an AWS CodePipeline pipeline as a continuous integration and continuous delivery (CI/C",
    "options": [
      "mechanism for a web application. A developer has written unit tests to programmatically test the functionality of the application code. The unit tests produce a test report that shows the results of each individual check. The developer now wants to run these tests automatically during the CI/CD process.",
      "Write a Git pre-commit hook that runs the test before every commi",
      "Ensure that each developer who is working on the project has the pre-commit hook instated locall",
      "Review the test report and resolve any issues before pushing changes to AWS CodeCommit.",
      "Add a new stage to the pipelin"
    ],
    "answer": "Ensure that each developer who is working on the project has the pre-commit hook instated locall",
    "explanation": "The solution that will meet the requirements is to add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage before the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues. This way, the developer can run the unit tests automatically during the CI/CD process and catch any bugs before deploying to the test environment. The developer can also use the test reports feature of CodeBuild to view and analyze the test results in a graphical interface. The other options either involve running the tests manually, running them after deployment, or using a different provider that requires additional configuration and integration. Reference: Test reports for CodeBuild"
  },
  {
    "question": "A company is developing an ecommerce application that uses Amazon API Gateway APIs. The application uses AWS Lambda as a backend. The company needs to test the code in a dedicated, monitored test environment before the company releases the code to the production environment. When solution will meet these requirements?",
    "options": [
      "Use a single stage in API Gatewa",
      "Create a Lambda function for each environmen",
      "Configure API clients to send a query parameter that indicates the endowment and the specific lambda function.",
      "Use multiple stages in API Gatewa"
    ],
    "answer": "Configure API clients to send a query parameter that indicates the endowment and the specific lambda function.",
    "explanation": "The solution that will meet the requirements is to use multiple stages in API Gateway. Create a Lambda function for each environment. Configure API Gateway stage variables to route traffic to the Lambda function in different environments. This way, the company can test the code in a dedicated, monitored test environment before releasing it to the production environment. The company can also use stage variables to specify the Lambda function version or alias for each stage, and avoid hard-coding the Lambda function name in the API Gateway integration. The other options either involve using a single stage in API Gateway, which does not allow testing in different environments, or adding different code blocks for different environments in the Lambda function, which increases complexity and maintenance. Reference: Set up stage variables for a REST API in API Gateway"
  },
  {
    "question": "A developer wants to store information about movies. Each movie has a title, release year, and genre. The movie information also can include additional properties about the cast and production crew. This additional information is inconsistent across movies. For example, one movie might have an assistant director, and another movie might have an animal trainer. The developer needs to implement a solution to support the following use cases: For a given title and release year, get all details about the movie that has that title and release year. For a given title, get all details about all movies that have that title. For a given genre, get all details about all movies in that genre. Which data store configuration will meet these requirements?",
    "options": [
      "Create an Amazon DynamoDB tabl",
      "Configure the table with a primary key that consists of the title as the partition key and the release year as the sort ke",
      "Create a global secondary index that uses the genre as the partition key and the title as the sort key.",
      "Create an Amazon DynamoDB tabl"
    ],
    "answer": "Create an Amazon DynamoDB tabl",
    "explanation": "Amazon DynamoDB is a fully managed NoSQL database service that provides fast and consistent performance with seamless scalability. The developer can create a DynamoDB table and configure the table with a primary key that consists of the title as the partition key and the release year as the sort key. This will enable querying for a given title and release year efficiently. The developer can also create a global secondary index that uses the genre as the partition key and the title as the sort key. This will enable querying for a given genre efficiently. The developer can store additional properties about the cast and production crew as attributes in the DynamoDB table. These attributes can have different data types and structures, and they do not need to be consistent across items."
  },
  {
    "question": "When using the AWS Encryption SDK how does the developer keep track of the data encryption keys used to encrypt data?",
    "options": [
      "The developer must manually keep Hack of the data encryption keys used for each data object.",
      "The SDK encrypts the data encryption key and stores it (encrypted) as part of the resumed ophertext.",
      "The SDK stores the data encryption keys automaticity in Amazon S3.",
      "The data encryption key is stored m the user data for the EC2 instance."
    ],
    "answer": "The SDK encrypts the data encryption key and stores it (encrypted) as part of the resumed ophertext.",
    "explanation": "This solution will meet the requirements by using AWS Encryption SDK, which is a client-side encryption library that enables developers to encrypt and decrypt data using data encryption keys that are protected by AWS Key Management Service (AWS KMS). The SDK encrypts the data encryption key with a customer master key (CMK) that is managed by AWS KMS, and stores it (encrypted) as part of the returned ciphertext. The developer does not need to keep track of the data encryption keys used to encrypt data, as they are stored with the encrypted data and can be retrieved and decrypted by using AWS KMS when needed. Option A is not optimal because it will require manual tracking of the data encryption keys used for each data object, which is error-prone and inefficient. Option C is not optimal because it will store the data encryption keys automatically in Amazon S3, which is unnecessary and insecure as Amazon S3 is not designed for storing encryption keys. Option D is not optimal because it wi"
  },
  {
    "question": "A developer is modifying an existing AWS Lambda function White checking the code the developer notices hardcoded parameter various for an Amazon RDS for SQL Server user name password database host and port. There also are hardcoded parameter values for an Amazon DynamoOB table. an Amazon S3 bucket, and an Amazon Simple Notification Service (Amazon SNS) topic. The developer wants to securely store the parameter values outside the code m an encrypted format and wants to turn on rotation for the credentials. The developer also wants to be able to reuse the parameter values from other applications and to update the parameter values without modifying code. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "Create an RDS database secret in AWS Secrets Manage",
      "Set the user name password, database, host and por",
      "Turn on secret rotatio",
      "Create encrypted Lambda environment variables for the DynamoDB table, S3 bucket and SNS topic.",
      "Create an RDS database secret in AWS Secrets Manage. Set the user name password, database, host and por. Turn on secret rotatio. Create Secure String parameters in AWS Systems Manager Parameter Store for the DynamoDB table, S3 bucket and SNS topic.. Create RDS database parameters in AWS Systems Manager Paramete"
    ],
    "answer": "Set the user name password, database, host and por",
    "explanation": "This solution will meet the requirements by using AWS Secrets Manager and AWS Systems Manager Parameter Store to securely store the parameter values outside the code in an encrypted format. AWS Secrets Manager is a service that helps protect secrets such as database credentials by encrypting them with AWS Key Management Service (AWS KMS) and enabling automatic rotation of secrets. The developer can create an RDS database secret in AWS Secrets Manager and set the user name, password, database, host, and port for accessing the RDS database. The developer can also turn on secret rotation, which will change the database credentials periodically according to a specified schedule or event. AWS Systems Manager Parameter Store is a service that provides secure and scalable storage for configuration data and secrets. The developer can create Secure String parameters in AWS Systems Manager Parameter Store for the DynamoDB table, S3 bucket, and SNS topic, which will encrypt them with AWS KMS. The"
  },
  {
    "question": "A company needs to distribute firmware updates to its customers around the world. Which service will allow easy and secure control of the access to the downloads at the lowest cost?",
    "options": [
      "Use Amazon CloudFront with signed URLs for Amazon S3.",
      "Create a dedicated Amazon CloudFront Distribution for each customer.",
      "Use Amazon CloudFront with AWS Lambda@Edge.",
      "Use Amazon API Gateway and AWS Lambda to control access to an S3 bucket."
    ],
    "answer": "Use Amazon CloudFront with signed URLs for Amazon S3.",
    "explanation": "This solution allows easy and secure control of access to the downloads at the lowest cost because it uses a content delivery network (CDN) that can cache and distribute firmware updates to customers around the world, and uses a mechanism that can restrict access to specific files or versions. Amazon CloudFront is a CDN that can improve performance, availability, and security of web applications by delivering content from edge locations closer to customers. Amazon S3 is a storage service that can store firmware updates in buckets and objects. Signed URLs are URLs that include additional information, such as an expiration date and time, that give users temporary access to specific objects in S3 buckets. The developer can use CloudFront to serve firmware updates from S3 buckets and use signed URLs to control who can download them and for how long. Creating a dedicated CloudFront distribution for each customer will incur unnecessary costs and complexity. Using Amazon CloudFront with AWS L"
  },
  {
    "question": "A company is using Amazon OpenSearch Service to implement an audit monitoring system. A developer needs to create an AWS Cloudformation custom resource that is associated with an AWS Lambda function to configure the OpenSearch Service domain. The Lambda function must access the OpenSearch Service domain by using Open Search Service internal master user credentials. What is the MOST secure way to pass these credentials to the Lambdas function?",
    "options": [
      "Use a CloudFormation parameter to pass the master user credentials at deployment to the OpenSearch Service domain's MasterUserOptions and the Lambda function's environment variabl",
      "Set the No Echo attenuate to true.",
      "Use a CloudFormation parameter to pass the master user credentials at deployment to the OpenSearch Service domain's MasterUserOptions and to create a paramete",
      "In AWS Systems Manager Parameter Stor"
    ],
    "answer": "In AWS Systems Manager Parameter Stor",
    "explanation": "The solution that will meet the requirements is to use CloudFormation to create an AWS Secrets Manager secret. Use a CloudFormation dynamic reference to retrieve the secret’s value for the OpenSearch Service domain’s MasterUserOptions. Create an IAM role that has the secretsmanager:GetSecretValue permission. Assign the role to the Lambda function. Store the secret’s name as the Lambda function’s environment variable. Resolve the secret’s value at runtime. This way, the developer can pass the credentials to the Lambda function in a secure way, as AWS Secrets Manager encrypts and manages the secrets. The developer can also use a dynamic reference to avoid exposing the secret’s value in plain text in the CloudFormation template. The other options either involve passing the credentials as plain text parameters, which is not secure, or encrypting them with AWS KMS, which is less convenient than using AWS Secrets Manager. Reference: Using dynamic references to specify template values"
  },
  {
    "question": "A company runs an application on AWS The application uses an AWS Lambda function that is configured with an Amazon Simple Queue Service (Amazon SQS) queue called high priority queue as the event source A developer is updating the Lambda function with another SQS queue called low priority queue as the event source The Lambda function must always read up to 10 simultaneous messages from the high priority queue before processing messages from low priority queue. The Lambda function must be limited to 100 simultaneous invocations. Which solution will meet these requirements'?",
    "options": [
      "Set the event source mapping batch size to 10 for the high priority queue and to 90 for the low priority queue",
      "Set the delivery delay to 0 seconds for the high priority queue and to 10 seconds for the low priority queue",
      "Set the event source mapping maximum concurrency to 10 for the high priority queue and to 90 for the low priority queue",
      "Set the event source mapping batch window to 10 for the high priority queue and to 90 for the low priority queue Your Partner of IT Exam We recommend you to try the PREMIUM DVA-C02 Dumps From Exambible (127 Q&As)"
    ],
    "answer": "Set the event source mapping maximum concurrency to 10 for the high priority queue and to 90 for the low priority queue",
    "explanation": "Setting the event source mapping maximum concurrency is the best way to control how many messages from each queue are processed by the Lambda function processed concurrently from the same event at a time. The maximum concurrency setting limits the number of batches that can be source. By setting it to 10 for the high priority queue and to 90 for the low priority queue, the developer can ensure that the Lambda function always reads up to 10 simultaneous messages from the high priority queue before processing messages from the low priority queue, and that the total number of concurrent invocations does not exceed 100. The other solutions are either not effective or not relevant. The batch size setting controls how many messages are sent to the Lambda function in a single invocation, not how many invocations are allowed at a time. The delivery delay setting controls how long a message is invisible in the queue after it is sent, not how often it is processed by the Lambda function. The bat"
  },
  {
    "question": "A developer is optimizing an AWS Lambda function and wants to test the changes in production on a small percentage of all traffic. The Lambda function serves requests to a REST API in Amazon API Gateway. The developer needs to deploy their changes and perform a test in production without changing the API Gateway URL. Which solution will meet these requirements?",
    "options": [
      "Define a function version for the currently deployed production Lambda functio",
      "Update the API Gateway endpoint to reference the new Lambda function versio",
      "Upload and publish the optimized Lambda function cod",
      "On the production API Gateway stage, define a canary release and set the percentage of traffic to direct to the canary releas"
    ],
    "answer": "Upload and publish the optimized Lambda function cod",
    "explanation": "? A Lambda alias is a pointer to a specific Lambda function version or another alias1. A Lambda alias allows you to invoke different versions of a function using the same name1. You can also split traffic between two aliases by assigning weights to them1. ? In this scenario, the developer needs to test their changes in production on a small percentage of all traffic without changing the API Gateway URL. To achieve this, the developer can follow these steps: ? By using this solution, the developer can test their changes in production on a small percentage of all traffic without changing the API Gateway URL. The developer can also monitor and compare metrics between the canary and production releases, and promote or disable the canary as needed2."
  },
  {
    "question": "A developer has observed an increase in bugs in the AWS Lambda functions that a development team has deployed in its Node is application. To minimize these bugs, the developer wants to impendent automated testing of Lambda functions in an environment that Closely simulates the Lambda environment. The developer needs to give other developers the ability to run the tests locally. The developer also needs to integrate the tests into the team's continuous integration and continuous delivery (Ct/CO) pipeline before the AWS Cloud Development Kit (AWS COK) deployment. Which solution will meet these requirements?",
    "options": [
      "Create sample events based on the Lambda documentatio",
      "Create automated test scripts that use the cdk local invoke command to invoke the Lambda function",
      "Check the response Document the test scripts for the other developers on the team Update the CI/CD pipeline to run the test scripts.",
      "Install a unit testing framework that reproduces the Lambda execution environmen"
    ],
    "answer": "Check the response Document the test scripts for the other developers on the team Update the CI/CD pipeline to run the test scripts.",
    "explanation": "This solution will meet the requirements by using AWS SAM CLI tool, which is a command line tool that lets developers locally build, test, debug, and deploy serverless applications defined by AWS SAM templates. The developer can use sam local generate- event command to generate sample events for different event sources such as API Gateway or S3. The developer can create automated test scripts that use sam local invoke command to invoke Lambda functions locally in an environment that closely simulates Lambda environment. The developer can check the response from Lambda functions and document how to run the test scripts for other developers on the team. The developer can also update CI/CD pipeline to run these test scripts before deploying with AWS CDK. Option A is not optimal because it will use cdk local invoke command, which does not exist in AWS CDK CLI tool. Option B is not optimal because it will use a unit testing framework that reproduces Lambda execution environment, which may n"
  },
  {
    "question": "A company uses Amazon API Gateway to expose a set of APIs to customers. The APIs have caching enabled in API Gateway. Customers need a way to invalidate the cache for each API when they test the API. What should a developer do to give customers the ability to invalidate the API cache?",
    "options": [
      "Ask the customers to use AWS credentials to call the InvalidateCache API operation.",
      "Attach an InvalidateCache policy to the IAM execution role that the customers use to invoke the AP",
      "Ask the customers to send a request that contains the HTTP header when they make an API call.",
      "Ask the customers to use the AWS SDK API Gateway class to invoke the InvalidateCache API operation.",
      "Attach an InvalidateCache policy to the IAM execution role that the customers use to invoke the AP. Ask the customers to add the INVALIDATE_CACHE query string parameter when they make an API call."
    ],
    "answer": "Ask the customers to use the AWS SDK API Gateway class to invoke the InvalidateCache API operation.",
    "explanation": "No explanation provided."
  },
  {
    "question": "A developer creates a static website for their department The developer deploys the static assets for the website to an Amazon S3 bucket and serves the assets with Amazon CloudFront The developer uses origin access control (OA",
    "options": [
      "on the CloudFront distribution to access the S3 bucket The developer notices users can access the root URL and specific pages but cannot access directories without specifying a file name. For example, /products/index.html works, but /products returns an error The developer needs to enable accessing directories without specifying a file name without exposing the S3 bucket publicly. Which solution will meet these requirements'?",
      "Update the CloudFront distribution's settings to index.html as the default root object is set Update the Amazon S3 bucket settings and enable static website hostin",
      "C. Specify index html as the Index document Update the S3 bucket policy to enable acces",
      "Update the CloudFront distribution's origin to use the S3 website endpoint"
    ],
    "answer": "on the CloudFront distribution to access the S3 bucket The developer notices users can access the root URL and specific pages but cannot access directories without specifying a file name. For example, /products/index.html works, but /products returns an error The developer needs to enable accessing directories without specifying a file name without exposing the S3 bucket publicly. Which solution will meet these requirements'?",
    "explanation": "The simplest and most efficient way to enable accessing directories without specifying a file name is to update the CloudFront distribution’s settings to index.html as the default root object. This will instruct CloudFront to return the index.html object when a user requests the root URL or a directory URL for the distribution. This solution does not require enabling static website hosting on the S3 bucket, creating a CloudFront function, or creating a custom error response. References ? Specifying a default root object ? cloudfront-default-root-object-configured ? How to setup CloudFront default root object? ? Ensure a default root object is configured for AWS Cloudfront …"
  },
  {
    "question": "A company is building a web application on AWS. When a customer sends a request, the application will generate reports and then make the reports available to the customer within one hour. Reports should be accessible to the customer for 8 hours. Some reports are larger than 1 M",
    "options": [
      "Each report is unique to the customer. The application should delete all reports that are older than 2 days. Which solution will meet these requirements with the LEAST operational overhead?",
      "Generate the reports and then store the reports as Amazon DynamoDB items that have a specified TT",
      "Generate a URL that retrieves the reports from DynamoD",
      "Provide the URL to customers through the web application.",
      "Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryptio"
    ],
    "answer": "Generate a URL that retrieves the reports from DynamoD",
    "explanation": "This solution will meet the requirements with the least operational overhead because it uses Amazon S3 as a scalable, secure, and durable storage service for the reports. The presigned URL will allow customers to access their reports for a limited time (8 hours) without requiring additional authentication. The S3 Lifecycle configuration rules will automatically delete the reports that are older than 2 days, reducing storage costs and complying with the data retention policy. Option A is not optimal because it will incur additional costs and complexity to store the reports as DynamoDB items, which have a size limit of 400 KB. Option B is not optimal because it will not provide customers with access to their reports within one hour, as Amazon SNS email delivery is not guaranteed. Option D is not optimal because it will require more operational overhead to manage an RDS database and a Lambda function for storing and deleting the reports."
  },
  {
    "question": "A developer uses AWS CloudFormation to deploy an Amazon API Gateway API and an AWS Step Functions state machine The state machine must reference the API Gateway API after the CloudFormation template is deployed The developer needs a solution that uses the state machine to reference the API Gateway endpoint. Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "Configure the CloudFormation template to reference the API endpoint in the DefinitionSubstitutions property for the AWS StepFunctions StateMachme resource.",
      "Configure the CloudFormation template to store the API endpoint in an environment variable for the AWS::StepFunctions::StateMachine resourc Configure the state machine to reference the environment variable",
      "Configure the CloudFormation template to store the API endpoint in a standard AWS: SecretsManager Secret resource Configure the state machine to reference the resource",
      "Configure the CloudFormation template to store the API endpoint in a standard AWS::AppConfig;:ConfigurationProfile resource Configure the state machine to reference the resource."
    ],
    "answer": "Configure the CloudFormation template to reference the API endpoint in the DefinitionSubstitutions property for the AWS StepFunctions StateMachme resource.",
    "explanation": "The most cost-effective solution is to use the DefinitionSubstitutions property of the AWS::StepFunctions::StateMachine resource to inject the API endpoint as a variable in the state machine definition. This way, the developer can use the intrinsic function Fn::GetAtt to get the API endpoint from the AWS::ApiGateway::RestApi resource, and pass it to the state machine without creating any additional resources or environment variables. The other solutions involve creating and managing extra resources, such as Secrets Manager secrets or AppConfig configuration profiles, which incur additional costs and complexity. References ? AWS::StepFunctions::StateMachine - AWS CloudFormation ? Call API Gateway with Step Functions - AWS Step Functions ? amazon-web-services aws-api-gateway terraform aws-step-functions"
  },
  {
    "question": "A company is migrating legacy internal applications to AWS. Leadership wants to rewrite the internal employee directory to use native AWS services. A developer needs to create a solution for storing employee contact details and high-resolution photos for use with the new application. Which solution will enable the search and retrieval of each employee's individual details and high-resolution photos using AWS APIs?",
    "options": [
      "Encode each employee's contact information and photos using Base64. Store the information in an Amazon DynamoDB table using a sort key.",
      "Store each employee's contact information in an Amazon DynamoDB table along with the object keys for the photos stored in Amazon S3. software-as-a-service (SaaS) method.",
      "Use Amazon Cognito user pools to implement the employee directory in a fully managed",
      "Store employee contact information in an Amazon RDS DB instance with the photos stored in Amazon Elastic File System (Amazon EFS)."
    ],
    "answer": "Store each employee's contact information in an Amazon DynamoDB table along with the object keys for the photos stored in Amazon S3. software-as-a-service (SaaS) method.",
    "explanation": "Amazon DynamoDB is a fully managed NoSQL database service that provides fast and consistent performance with seamless scalability. The developer can store each employee’s contact information in a DynamoDB table along with the object keys for the photos stored in Amazon S3. Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. The developer can use AWS APIs to search and retrieve the employee details and photos from DynamoDB and S3."
  },
  {
    "question": "A developer at a company needs to create a small application mat makes the same API call once each flay at a designated time. The company does not have infrastructure in the AWS Cloud yet, but the company wants to implement this functionality on AWS. Which solution meets these requirements in the MOST operationally efficient manner?",
    "options": [
      "Use a Kubermetes cron job that runs on Amazon Elastic Kubemetes Sen/ice (Amazon EKS)",
      "Use an Amazon Linux crontab scheduled job that runs on Amazon EC2",
      "Use an AWS Lambda function that is invoked by an Amazon EventBridge scheduled event.",
      "Use an AWS Batch job that is submitted to an AWS Batch job queue."
    ],
    "answer": "Use an AWS Lambda function that is invoked by an Amazon EventBridge scheduled event.",
    "explanation": "This solution meets the requirements in the most operationally efficient manner because it does not require any infrastructure provisioning or management. The developer can create a Lambda function that makes the API call and configure an EventBridge rule that triggers the function once a day at a designated time. This is a serverless solution that scales automatically and only charges for the execution time of the function. Reference: [Using AWS Lambda with Amazon EventBridge], [Schedule Expressions for Rules]"
  },
  {
    "question": "A developer accesses AWS CodeCommit over SSH. The SSH keys configured to access AWS CodeCommit are tied to a user with the following permissions: Your Partner of IT Exam We recommend you to try the PREMIUM DVA-C02 Dumps From Exambible (127 Q&As) The developer needs to create/delete branches Which specific IAM permissions need to be added based on the principle of least privilege?",
    "options": [
      "Option A",
      "Option B",
      "Option C",
      "Option D"
    ],
    "answer": "Option A",
    "explanation": "This solution allows the developer to create and delete branches in AWS CodeCommit by granting the codecommit:CreateBranch and codecommit:DeleteBranch permissions. These are the minimum permissions required for this task, following the principle of least privilege. Option B grants too many permissions, such codecommit:Put*, which allows the developer to create, update, or delete any resource in CodeCommit. Option C grants too few as permissions, such as codecommit:Update*, which does not allow the developer to create or delete branches. Option D grants all permissions, such as codecommit:*, which is not secure or recommended. Reference: [AWS CodeCommit Permissions Reference], [Create a Branch (AWS CLI)]"
  },
  {
    "question": "What is the minimum memory allocation for a Lambda function?",
    "options": [
      "64 MB",
      "128 MB",
      "256 MB",
      "512 MB"
    ],
    "answer": "128 MB",
    "explanation": "Lambda functions can be configured with memory from 128 MB to 10,240 MB in 1 MB increments."
  },
  {
    "question": "How does Lambda charge for compute time?",
    "options": [
      "Per invocation only",
      "Per GB-second",
      "Per hour",
      "Flat monthly rate"
    ],
    "answer": "Per GB-second",
    "explanation": "Lambda charges based on the number of requests and the duration (GB-second) your code executes."
  },
  {
    "question": "What is a Lambda Layer?",
    "options": [
      "A networking component",
      "A shared code/library package",
      "A deployment stage",
      "An execution environment"
    ],
    "answer": "A shared code/library package",
    "explanation": "Lambda Layers let you manage common dependencies separately from your function code."
  },
  {
    "question": "Which environment variable contains the Lambda function name?",
    "options": [
      "LAMBDA_NAME",
      "AWS_LAMBDA_FUNCTION_NAME",
      "FUNCTION_NAME",
      "AWS_FUNCTION"
    ],
    "answer": "AWS_LAMBDA_FUNCTION_NAME",
    "explanation": "AWS_LAMBDA_FUNCTION_NAME is an automatically set environment variable containing the function name."
  },
  {
    "question": "What is Lambda's /tmp directory size limit?",
    "options": [
      "256 MB",
      "512 MB",
      "1 GB",
      "10 GB"
    ],
    "answer": "512 MB",
    "explanation": "Lambda provides 512 MB of /tmp disk space for temporary files during execution."
  },
  {
    "question": "How can you make a Lambda function respond faster to requests?",
    "options": [
      "Increase memory",
      "Use provisioned concurrency",
      "Add more environment variables",
      "Use larger /tmp space"
    ],
    "answer": "Use provisioned concurrency",
    "explanation": "Provisioned concurrency keeps functions initialized and ready to respond in milliseconds."
  },
  {
    "question": "What is the default concurrent execution limit for Lambda per region?",
    "options": [
      "100",
      "500",
      "1,000",
      "10,000"
    ],
    "answer": "1,000",
    "explanation": "The default concurrent execution quota is 1,000 per region, but can be increased via service quotas."
  },
  {
    "question": "Which Lambda runtime is NOT officially supported by AWS?",
    "options": [
      "Node.js",
      "Python",
      "PHP",
      "Ruby"
    ],
    "answer": "PHP",
    "explanation": "AWS Lambda officially supports Node.js, Python, Ruby, Java, Go, .NET Core, but not PHP natively."
  },
  {
    "question": "How can you pass configuration data to a Lambda function?",
    "options": [
      "Environment variables",
      "Event payload",
      "Context object",
      "All of the above"
    ],
    "answer": "All of the above",
    "explanation": "Configuration data can be passed via environment variables, event payload, or context object."
  },
  {
    "question": "What happens if a Lambda function exceeds its timeout?",
    "options": [
      "It continues running",
      "It's terminated and returns an error",
      "It pauses",
      "It restarts"
    ],
    "answer": "It's terminated and returns an error",
    "explanation": "When timeout is reached, Lambda terminates the function execution and returns a timeout error."
  },
  {
    "question": "What is the maximum size of a DynamoDB item?",
    "options": [
      "100 KB",
      "256 KB",
      "400 KB",
      "1 MB"
    ],
    "answer": "400 KB",
    "explanation": "The maximum item size in DynamoDB is 400 KB, including attribute names and values."
  },
  {
    "question": "What is DynamoDB's default consistency model?",
    "options": [
      "Strong",
      "Eventual",
      "Immediate",
      "Causal"
    ],
    "answer": "Eventual",
    "explanation": "DynamoDB uses eventual consistency by default for read operations, offering better performance."
  },
  {
    "question": "How many global secondary indexes can a DynamoDB table have?",
    "options": [
      "5",
      "10",
      "20",
      "Unlimited"
    ],
    "answer": "20",
    "explanation": "You can create up to 20 global secondary indexes per table."
  },
  {
    "question": "What is the difference between Query and Scan in DynamoDB?",
    "options": [
      "Query is faster and more efficient",
      "Scan is always better",
      "They're the same",
      "Query requires full table access"
    ],
    "answer": "Query is faster and more efficient",
    "explanation": "Query uses indexes and is much more efficient than Scan which reads the entire table."
  },
  {
    "question": "What is DynamoDB Accelerator (DAX)?",
    "options": [
      "A backup service",
      "An in-memory cache",
      "A replication tool",
      "A monitoring service"
    ],
    "answer": "An in-memory cache",
    "explanation": "DAX is a fully managed, in-memory cache for DynamoDB that delivers microsecond response times."
  },
  {
    "question": "How can you implement optimistic locking in DynamoDB?",
    "options": [
      "Using conditional writes",
      "Using pessimistic locks",
      "Using table locks",
      "It's automatic"
    ],
    "answer": "Using conditional writes",
    "explanation": "Optimistic locking is implemented using conditional writes based on an attribute like version number."
  },
  {
    "question": "What billing model does DynamoDB offer?",
    "options": [
      "Provisioned capacity only",
      "On-demand only",
      "Both provisioned and on-demand",
      "Free tier only"
    ],
    "answer": "Both provisioned and on-demand",
    "explanation": "DynamoDB offers both provisioned capacity and on-demand (pay-per-request) billing models."
  },
  {
    "question": "What is a DynamoDB partition key?",
    "options": [
      "A unique identifier",
      "The primary hash key for data distribution",
      "A sort key",
      "An index"
    ],
    "answer": "The primary hash key for data distribution",
    "explanation": "The partition key is used to distribute data across partitions for scalability and performance."
  },
  {
    "question": "How does DynamoDB handle capacity when using on-demand mode?",
    "options": [
      "You set fixed capacity",
      "It auto-scales based on traffic",
      "Manual scaling only",
      "Limited to 100 RCUs"
    ],
    "answer": "It auto-scales based on traffic",
    "explanation": "On-demand mode automatically scales to accommodate your workload with no capacity planning."
  },
  {
    "question": "What is a Local Secondary Index (LSI)?",
    "options": [
      "An index with a different partition key",
      "An index with the same partition key but different sort key",
      "A global index",
      "A cached index"
    ],
    "answer": "An index with the same partition key but different sort key",
    "explanation": "LSIs use the same partition key as the base table but have a different sort key."
  },
  {
    "question": "What are the three types of API Gateway endpoints?",
    "options": [
      "Regional, Edge-optimized, Private",
      "Public, Private, Hybrid",
      "Standard, Premium, Enterprise",
      "HTTP, REST, GraphQL"
    ],
    "answer": "Regional, Edge-optimized, Private",
    "explanation": "API Gateway supports Regional, Edge-optimized (using CloudFront), and Private (VPC) endpoints."
  },
  {
    "question": "How can you reduce API Gateway costs?",
    "options": [
      "Enable caching",
      "Use resource policies",
      "Increase throttling",
      "Disable logging"
    ],
    "answer": "Enable caching",
    "explanation": "Enabling caching reduces the number of calls to your backend, lowering costs and improving performance."
  },
  {
    "question": "What is API Gateway request validation?",
    "options": [
      "Checking IAM permissions",
      "Validating request parameters and body before invoking backend",
      "Logging requests",
      "Rate limiting"
    ],
    "answer": "Validating request parameters and body before invoking backend",
    "explanation": "Request validation checks the request before forwarding to the backend, reducing invalid requests."
  },
  {
    "question": "How do you implement API versioning in API Gateway?",
    "options": [
      "Using stages",
      "Using resource paths",
      "Using domain names",
      "All of the above"
    ],
    "answer": "All of the above",
    "explanation": "API versioning can be implemented using stages, resource paths (v1/v2), or custom domain names."
  },
  {
    "question": "What is a Lambda authorizer in API Gateway?",
    "options": [
      "A Lambda function that controls access to APIs",
      "A deployment tool",
      "A monitoring service",
      "A caching layer"
    ],
    "answer": "A Lambda function that controls access to APIs",
    "explanation": "Lambda authorizers (formerly custom authorizers) use Lambda functions to control API access."
  },
  {
    "question": "What is the maximum timeout for API Gateway?",
    "options": [
      "10 seconds",
      "30 seconds",
      "60 seconds",
      "5 minutes"
    ],
    "answer": "30 seconds",
    "explanation": "API Gateway has a maximum integration timeout of 30 seconds (29 seconds for non-proxy integrations)."
  },
  {
    "question": "How can you enable CORS in API Gateway?",
    "options": [
      "Add CORS headers in integration response",
      "Enable CORS in console",
      "Use OPTIONS method",
      "All of the above"
    ],
    "answer": "All of the above",
    "explanation": "CORS requires proper headers, enabling in console, and handling OPTIONS preflight requests."
  },
  {
    "question": "What is API Gateway's throttling burst limit?",
    "options": [
      "1,000 requests",
      "5,000 requests",
      "10,000 requests",
      "Unlimited"
    ],
    "answer": "5,000 requests",
    "explanation": "The default burst limit is 5,000 requests across all APIs in an AWS account."
  },
  {
    "question": "What format does API Gateway use for OpenAPI?",
    "options": [
      "YAML or JSON",
      "XML only",
      "Proprietary format",
      "Binary"
    ],
    "answer": "YAML or JSON",
    "explanation": "API Gateway supports OpenAPI 3.0 specifications in both YAML and JSON formats."
  },
  {
    "question": "How can you test API Gateway before deployment?",
    "options": [
      "Using the Test feature in console",
      "Using curl",
      "Using Postman",
      "All of the above"
    ],
    "answer": "All of the above",
    "explanation": "You can test using the built-in console tester, curl, Postman, or any HTTP client."
  },
  {
    "question": "What is S3 Transfer Acceleration?",
    "options": [
      "Faster uploads via CloudFront edge locations",
      "Parallel uploads",
      "Compressed transfers",
      "Direct VPC access"
    ],
    "answer": "Faster uploads via CloudFront edge locations",
    "explanation": "Transfer Acceleration uses CloudFront's edge locations to accelerate uploads to S3."
  },
  {
    "question": "What is an S3 presigned URL used for?",
    "options": [
      "Permanent public access",
      "Temporary access to private objects",
      "Faster downloads",
      "Encryption"
    ],
    "answer": "Temporary access to private objects",
    "explanation": "Presigned URLs grant time-limited access to private S3 objects without changing permissions."
  },
  {
    "question": "What is EC2 user data?",
    "options": [
      "Metadata about the instance",
      "Scripts that run at instance launch",
      "User login credentials",
      "Instance configuration"
    ],
    "answer": "Scripts that run at instance launch",
    "explanation": "User data allows you to run scripts or commands when an EC2 instance first starts."
  },
  {
    "question": "What is the difference between ECS and EKS?",
    "options": [
      "ECS is AWS-native, EKS is Kubernetes-based",
      "They're the same",
      "EKS is for Lambda only",
      "ECS doesn't support containers"
    ],
    "answer": "ECS is AWS-native, EKS is Kubernetes-based",
    "explanation": "ECS is AWS's proprietary container orchestration, while EKS is managed Kubernetes."
  },
  {
    "question": "What is a CloudFormation nested stack?",
    "options": [
      "A stack within another stack",
      "A backup stack",
      "A testing environment",
      "A deleted stack"
    ],
    "answer": "A stack within another stack",
    "explanation": "Nested stacks allow you to create reusable templates and manage complex infrastructures."
  },
  {
    "question": "What is a CodePipeline stage?",
    "options": [
      "A logical unit containing actions",
      "A deployment environment",
      "A testing phase",
      "A version"
    ],
    "answer": "A logical unit containing actions",
    "explanation": "A stage is a logical unit that contains one or more actions in a pipeline."
  },
  {
    "question": "What is a CloudWatch custom metric?",
    "options": [
      "Metrics published by your application",
      "AWS service metrics",
      "Free metrics",
      "Real-time metrics"
    ],
    "answer": "Metrics published by your application",
    "explanation": "Custom metrics allow you to publish application-specific data points to CloudWatch."
  },
  {
    "question": "What is an X-Ray segment?",
    "options": [
      "A unit of work recorded by X-Ray",
      "A network segment",
      "A time period",
      "An error log"
    ],
    "answer": "A unit of work recorded by X-Ray",
    "explanation": "Segments represent units of work done by your application, such as HTTP requests."
  },
  {
    "question": "What is an AWS KMS Customer Master Key (CMK)?",
    "options": [
      "A password",
      "A logical representation of an encryption key",
      "A database key",
      "An API key"
    ],
    "answer": "A logical representation of an encryption key",
    "explanation": "A CMK is a logical representation of a master key in KMS used for encryption operations."
  },
  {
    "question": "How often can Secrets Manager rotate secrets?",
    "options": [
      "Daily",
      "Configurable (days to years)",
      "Annually only",
      "It doesn't rotate"
    ],
    "answer": "Configurable (days to years)",
    "explanation": "Rotation schedules are fully configurable based on your security requirements."
  },
  {
    "question": "What is Cognito Sync used for?",
    "options": [
      "Syncing user data across devices",
      "Authenticating users",
      "Password recovery",
      "MFA"
    ],
    "answer": "Syncing user data across devices",
    "explanation": "Cognito Sync enables cross-device sync of user profile data (now largely replaced by AppSync)."
  },
  {
    "question": "What is SQS visibility timeout?",
    "options": [
      "Time a message is hidden after being received",
      "Message expiration time",
      "Queue creation time",
      "Polling interval"
    ],
    "answer": "Time a message is hidden after being received",
    "explanation": "Visibility timeout prevents other consumers from receiving the same message while it's being processed."
  },
  {
    "question": "What is SNS fanout?",
    "options": [
      "Delivering messages to multiple subscribers",
      "Load balancing",
      "Message filtering",
      "Topic creation"
    ],
    "answer": "Delivering messages to multiple subscribers",
    "explanation": "Fanout pattern uses SNS to deliver messages to multiple SQS queues or endpoints simultaneously."
  },
  {
    "question": "What language are Step Functions state machines defined in?",
    "options": [
      "JSON-based Amazon States Language",
      "YAML",
      "Python",
      "JavaScript"
    ],
    "answer": "JSON-based Amazon States Language",
    "explanation": "Step Functions use Amazon States Language (ASL), a JSON-based structured language."
  },
  {
    "question": "What is Amazon EventBridge?",
    "options": [
      "A serverless event bus service",
      "A network bridge",
      "A database replication tool",
      "A monitoring service"
    ],
    "answer": "A serverless event bus service",
    "explanation": "EventBridge is a serverless event bus for connecting applications using events."
  },
  {
    "question": "Which caching strategy adds data to cache only when requested?",
    "options": [
      "Lazy loading",
      "Write-through",
      "Eager loading",
      "Pre-loading"
    ],
    "answer": "Lazy loading",
    "explanation": "Lazy loading (cache-aside) only caches data when it's first requested, avoiding unnecessary caching."
  },
  {
    "question": "What is RDS Multi-AZ used for?",
    "options": [
      "High availability and failover",
      "Read scaling",
      "Backup storage",
      "Performance optimization"
    ],
    "answer": "High availability and failover",
    "explanation": "Multi-AZ provides high availability with automatic failover to a standby instance."
  },
  {
    "question": "What is Amazon Aurora Serverless?",
    "options": [
      "An auto-scaling Aurora configuration",
      "A backup service",
      "A migration tool",
      "A monitoring service"
    ],
    "answer": "An auto-scaling Aurora configuration",
    "explanation": "Aurora Serverless automatically starts up, scales, and shuts down based on application needs."
  },
  {
    "question": "What does Elastic Beanstalk manage for you?",
    "options": [
      "Infrastructure provisioning and application deployment",
      "Only deployment",
      "Only monitoring",
      "Only scaling"
    ],
    "answer": "Infrastructure provisioning and application deployment",
    "explanation": "Elastic Beanstalk handles infrastructure provisioning, deployment, monitoring, and scaling."
  },
  {
    "question": "What is AWS SAM?",
    "options": [
      "Serverless Application Model for defining serverless apps",
      "A database service",
      "A monitoring tool",
      "A networking service"
    ],
    "answer": "Serverless Application Model for defining serverless apps",
    "explanation": "SAM is an open-source framework for building serverless applications on AWS."
  },
  {
    "question": "What triggers a Lambda function automatically?",
    "options": [
      "Manual invocation only",
      "Events from AWS services",
      "Cron jobs",
      "User requests"
    ],
    "answer": "Events from AWS services",
    "explanation": "Lambda can be triggered by events from S3, DynamoDB, SNS, SQS, API Gateway, and many other AWS services."
  },
  {
    "question": "What is Lambda reserved concurrency?",
    "options": [
      "Guarantees execution",
      "Sets maximum concurrent executions",
      "Increases memory",
      "Enables VPC access"
    ],
    "answer": "Sets maximum concurrent executions",
    "explanation": "Reserved concurrency sets the maximum number of concurrent instances for your function."
  },
  {
    "question": "How can you share code between Lambda functions?",
    "options": [
      "Copy-paste code",
      "Lambda Layers",
      "S3 bucket",
      "GitHub"
    ],
    "answer": "Lambda Layers",
    "explanation": "Lambda Layers allow you to manage and share code libraries across multiple functions."
  },
  {
    "question": "What is Lambda's init code?",
    "options": [
      "Code run on every invocation",
      "Code run once during cold start",
      "Configuration code",
      "Test code"
    ],
    "answer": "Code run once during cold start",
    "explanation": "Init code runs once when a new execution environment is created, not on every invocation."
  },
  {
    "question": "How can you invoke a Lambda function synchronously?",
    "options": [
      "Use Event invocation type",
      "Use RequestResponse invocation type",
      "Use DryRun",
      "Use Test"
    ],
    "answer": "Use RequestResponse invocation type",
    "explanation": "RequestResponse invocation type waits for the function to process the event and returns a response."
  },
  {
    "question": "What is Lambda's SnapStart feature?",
    "options": [
      "Faster cold starts for Java functions",
      "Scheduled execution",
      "Backup service",
      "Monitoring tool"
    ],
    "answer": "Faster cold starts for Java functions",
    "explanation": "SnapStart reduces cold start times for Java functions by reusing initialized snapshots."
  },
  {
    "question": "Can Lambda functions access resources in a VPC?",
    "options": [
      "No, never",
      "Yes, with VPC configuration",
      "Only EC2 instances",
      "Only RDS databases"
    ],
    "answer": "Yes, with VPC configuration",
    "explanation": "Lambda functions can be configured to access resources in your VPC by specifying subnets and security groups."
  },
  {
    "question": "What happens to Lambda environment variables at rest?",
    "options": [
      "Stored in plaintext",
      "Encrypted with AWS managed keys",
      "Not stored",
      "Hashed"
    ],
    "answer": "Encrypted with AWS managed keys",
    "explanation": "Lambda automatically encrypts environment variables at rest using AWS managed encryption keys."
  },
  {
    "question": "What is the Lambda execution context?",
    "options": [
      "The code itself",
      "Temporary runtime environment",
      "AWS region",
      "IAM role"
    ],
    "answer": "Temporary runtime environment",
    "explanation": "The execution context is a temporary runtime environment that initializes external dependencies."
  },
  {
    "question": "How can you monitor Lambda function performance?",
    "options": [
      "CloudWatch Metrics and Logs",
      "S3 logs only",
      "Email notifications",
      "SSH access"
    ],
    "answer": "CloudWatch Metrics and Logs",
    "explanation": "Lambda automatically sends metrics to CloudWatch and you can enable detailed logging."
  },
  {
    "question": "What is a DynamoDB composite primary key?",
    "options": [
      "Single partition key",
      "Partition key + sort key",
      "Multiple partition keys",
      "Foreign key"
    ],
    "answer": "Partition key + sort key",
    "explanation": "A composite primary key consists of a partition key and a sort key for more flexible querying."
  },
  {
    "question": "How can you perform atomic updates in DynamoDB?",
    "options": [
      "Use UpdateItem operation",
      "Delete and recreate",
      "Use transactions",
      "Both A and C"
    ],
    "answer": "Both A and C",
    "explanation": "DynamoDB supports atomic updates through UpdateItem and ACID transactions."
  },
  {
    "question": "What is DynamoDB's WCU (Write Capacity Unit)?",
    "options": [
      "1 read per second",
      "1 write per second for item up to 1 KB",
      "1 GB storage",
      "1 transaction"
    ],
    "answer": "1 write per second for item up to 1 KB",
    "explanation": "One WCU represents one write per second for an item up to 1 KB in size."
  },
  {
    "question": "What is a DynamoDB sparse index?",
    "options": [
      "Index with missing values",
      "Index with only some items having the indexed attribute",
      "Compressed index",
      "Partial index"
    ],
    "answer": "Index with only some items having the indexed attribute",
    "explanation": "Sparse indexes only include items that have the indexed attribute, reducing index size."
  },
  {
    "question": "How does DynamoDB handle hot partitions?",
    "options": [
      "Adaptive capacity",
      "Manual redistribution",
      "Deletes data",
      "Locks access"
    ],
    "answer": "Adaptive capacity",
    "explanation": "DynamoDB automatically applies adaptive capacity to handle hot partitions."
  },
  {
    "question": "What is DynamoDB Time to Live (TTL)?",
    "options": [
      "Session timeout",
      "Automatic item deletion after timestamp",
      "Query timeout",
      "Connection timeout"
    ],
    "answer": "Automatic item deletion after timestamp",
    "explanation": "TTL automatically deletes items after their TTL attribute timestamp expires."
  },
  {
    "question": "Can you use SQL with DynamoDB?",
    "options": [
      "No, NoSQL only",
      "Yes, via PartiQL",
      "Only with third-party tools",
      "Only for queries"
    ],
    "answer": "Yes, via PartiQL",
    "explanation": "DynamoDB supports PartiQL, an SQL-compatible query language."
  },
  {
    "question": "What is DynamoDB global table?",
    "options": [
      "Large table",
      "Multi-region, multi-master replication",
      "Shared table",
      "Backup table"
    ],
    "answer": "Multi-region, multi-master replication",
    "explanation": "Global tables provide fully managed multi-region, multi-active database replication."
  },
  {
    "question": "How can you backup DynamoDB tables?",
    "options": [
      "On-demand backups",
      "Point-in-time recovery",
      "Export to S3",
      "All of the above"
    ],
    "answer": "All of the above",
    "explanation": "DynamoDB supports on-demand backups, PITR, and exports to S3."
  },
  {
    "question": "What is a DynamoDB conditional write?",
    "options": [
      "Write with retry",
      "Write only if condition is met",
      "Scheduled write",
      "Batch write"
    ],
    "answer": "Write only if condition is met",
    "explanation": "Conditional writes only succeed if the specified condition evaluates to true."
  },
  {
    "question": "What is API Gateway mapping template?",
    "options": [
      "URL mapping",
      "Request/response transformation using VTL",
      "Domain mapping",
      "Resource mapping"
    ],
    "answer": "Request/response transformation using VTL",
    "explanation": "Mapping templates use Velocity Template Language (VTL) to transform requests and responses."
  },
  {
    "question": "What is API Gateway usage plan?",
    "options": [
      "Deployment plan",
      "Throttling and quota limits for API keys",
      "Pricing plan",
      "Monitoring plan"
    ],
    "answer": "Throttling and quota limits for API keys",
    "explanation": "Usage plans specify throttling and quota limits for API keys."
  },
  {
    "question": "How can you implement API Gateway canary deployments?",
    "options": [
      "Using stages",
      "Using canary settings on stages",
      "Manual traffic splitting",
      "Load balancer"
    ],
    "answer": "Using canary settings on stages",
    "explanation": "API Gateway canary settings route a percentage of traffic to a new deployment."
  },
  {
    "question": "What is API Gateway REST API vs HTTP API?",
    "options": [
      "REST has more features, HTTP is simpler and cheaper",
      "They're identical",
      "HTTP is deprecated",
      "REST is slower"
    ],
    "answer": "REST has more features, HTTP is simpler and cheaper",
    "explanation": "HTTP APIs are optimized for building APIs with lower latency and cost, while REST APIs have more features."
  },
  {
    "question": "Can API Gateway transform request bodies?",
    "options": [
      "No",
      "Yes, using mapping templates",
      "Only for Lambda",
      "Only for HTTP"
    ],
    "answer": "Yes, using mapping templates",
    "explanation": "Mapping templates allow you to transform request and response bodies."
  },
  {
    "question": "What is API Gateway request/response data mapping?",
    "options": [
      "URL rewriting",
      "Converting between formats and structures",
      "Load balancing",
      "Caching"
    ],
    "answer": "Converting between formats and structures",
    "explanation": "Data mapping transforms request/response payloads between client and backend formats."
  },
  {
    "question": "How can you version your API in API Gateway?",
    "options": [
      "Stage names",
      "Resource paths",
      "Custom domains",
      "All of the above"
    ],
    "answer": "All of the above",
    "explanation": "API versioning can use stages (v1, v2), paths (/v1/resource), or custom domains."
  },
  {
    "question": "What is API Gateway Binary Media Types?",
    "options": [
      "Image compression",
      "Support for binary payloads like images",
      "JSON only",
      "Text encoding"
    ],
    "answer": "Support for binary payloads like images",
    "explanation": "Binary media types allow API Gateway to handle binary data like images, PDFs, etc."
  },
  {
    "question": "What happens when API Gateway cache is enabled?",
    "options": [
      "Responses are cached",
      "Faster response times",
      "Reduced backend calls",
      "All of the above"
    ],
    "answer": "All of the above",
    "explanation": "Caching stores responses, improving performance and reducing backend load."
  },
  {
    "question": "What is API Gateway WebSocket API?",
    "options": [
      "HTTP protocol",
      "Two-way communication protocol",
      "REST replacement",
      "Deprecated feature"
    ],
    "answer": "Two-way communication protocol",
    "explanation": "WebSocket APIs enable two-way communication between client and server."
  },
  {
    "question": "What is S3 multipart upload?",
    "options": [
      "Upload in parts for large files",
      "Multiple files at once",
      "Parallel downloads",
      "Distributed storage"
    ],
    "answer": "Upload in parts for large files",
    "explanation": "Multipart upload allows uploading large objects in parts, improving reliability and performance."
  },
  {
    "question": "What is S3 event notification?",
    "options": [
      "Alerts for bucket activities",
      "Automatic triggers for Lambda/SNS/SQS",
      "Monitoring service",
      "Error notifications"
    ],
    "answer": "Automatic triggers for Lambda/SNS/SQS",
    "explanation": "S3 event notifications trigger Lambda functions, SNS topics, or SQS queues on bucket events."
  },
  {
    "question": "What is S3 Cross-Region Replication (CRR)?",
    "options": [
      "Automatic object replication across regions",
      "Manual backup",
      "Load balancing",
      "Content delivery"
    ],
    "answer": "Automatic object replication across regions",
    "explanation": "CRR automatically replicates objects to buckets in different AWS regions."
  },
  {
    "question": "What is S3 Object Lock?",
    "options": [
      "Password protection",
      "WORM (write once, read many) protection",
      "Access control",
      "Encryption"
    ],
    "answer": "WORM (write once, read many) protection",
    "explanation": "Object Lock prevents object deletion or modification for a specified retention period."
  },
  {
    "question": "What is S3 Intelligent-Tiering?",
    "options": [
      "Manual storage class changes",
      "Automatic cost optimization by moving objects between tiers",
      "Smart caching",
      "AI-powered search"
    ],
    "answer": "Automatic cost optimization by moving objects between tiers",
    "explanation": "Intelligent-Tiering automatically moves objects between access tiers based on usage patterns."
  },
  {
    "question": "What is S3 bucket policy?",
    "options": [
      "Storage size limit",
      "Resource-based access policy",
      "Pricing policy",
      "Replication policy"
    ],
    "answer": "Resource-based access policy",
    "explanation": "Bucket policies are resource-based policies that grant permissions to buckets and objects."
  },
  {
    "question": "What is S3 Access Points?",
    "options": [
      "Entry points for VPC access",
      "Unique endpoints for specific applications",
      "Network optimization",
      "Backup points"
    ],
    "answer": "Unique endpoints for specific applications",
    "explanation": "Access Points simplify managing data access for shared datasets with specific permissions."
  },
  {
    "question": "What is S3 Glacier Instant Retrieval?",
    "options": [
      "Millisecond retrieval for archive data",
      "Minutes retrieval",
      "Hours retrieval",
      "Days retrieval"
    ],
    "answer": "Millisecond retrieval for archive data",
    "explanation": "Glacier Instant Retrieval provides millisecond retrieval for rarely accessed archive data."
  },
  {
    "question": "What is S3 Server Access Logging?",
    "options": [
      "Error logs",
      "Detailed records of requests made to a bucket",
      "Server health logs",
      "Application logs"
    ],
    "answer": "Detailed records of requests made to a bucket",
    "explanation": "Server access logging provides detailed records for audit and compliance."
  },
  {
    "question": "What is S3 Inventory?",
    "options": [
      "Storage size report",
      "Scheduled report of objects and metadata",
      "Cost analysis",
      "Access logs"
    ],
    "answer": "Scheduled report of objects and metadata",
    "explanation": "S3 Inventory provides scheduled reports of your objects and their metadata."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for caching?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "ElastiCache",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for monitoring?",
    "options": [
      "Lambda",
      "CloudWatch"
    ],
    "answer": "CloudWatch",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for serverless compute?",
    "options": [
      "Lambda",
      "ECS",
      "CloudWatch"
    ],
    "answer": "Lambda",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for container orchestration?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "CloudWatch"
    ],
    "answer": "ECS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  },
  {
    "question": "Which AWS service is best for message queuing?",
    "options": [
      "Lambda",
      "ECS",
      "SQS",
      "ElastiCache",
      "CloudWatch"
    ],
    "answer": "SQS",
    "explanation": "This service provides the optimal solution for this use case in AWS."
  }
]